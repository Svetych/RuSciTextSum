Методы повышения обобщающей способности, основанные на построении многослойных ансамблей

При использовании алгоритмов машинного обучения очень часто возникает проблема «переобучения». В такой ситуации алгоритм начинает более точно работать на объектах из обучающей выборки, но при этом теряет качество работы на генеральной совокупности объектов, то есть его обобщающая способность падает.
Многочисленные исследования  показали, что ансамбли, агрегирующие результаты различных алгоритмов, могут повышать обобщающую способность в сравнении с отдельными алгоритмами. Тем не менее, в большинстве случаев агрегирование результатов нескольких алгоритмов происходит лишь один раз. В работе рассматриваются новые методы повышения качества обучения, использующие в своей структуре агрегирование несколько раз. Приводится способ создания многоуровневой схемы, позволяющей использовать различные методы агрегирования алгоритмов на различных уровнях схемы. Указанная схема во многом является обобщением многослойных нейросетевых моделей. Также в работе рассматриваются два частных варианта схемы. Первый вариант решает задачу классификации и использует двухуровневую структуру (или имеет два слоя). Первый слой состоит из деревьев решений, которые генерируются на случайно выбираемых подмножествах признаков и с использованием бэггинга . Второй слой включает в себя несколько логистических регрессий, каждая из которых получает на вход результаты работы небольшого количества случайно выбранных деревьев решений с первого слоя. Если на первом слое n деревьев, то эмпирически было получено, что наилучших результатов можно достигнуть, если вход каждой логистической регрессии связан лишь с vn случайно выбранных деревьев решений. Это позволяет достичь разнообразия итоговых логистических регрессий, что влечёт за собой повышение обобщающей способности . Выходы логистических регрессий усредняются и полученное число считается вероятностью принадлежности к некоторому классу. Таким образом, данная схема соответствует двухслойной нейронной сети с недифференцируемыми функциями активации, что позволяет лучше описывать локальные нелинейные зависимости данных. Для обучения полученной схемы необходимо разделить обучающую выборку на две части и обучать первый слой, состоящий из деревьев решений, на первой подвыборке, а второй слой — на второй.
Второй вариант схемы позволяет связать в одну модель деревья решений и нейронную сеть. Более конкретно, деревья решений объединяются в несколько ансамблей (в данном случае, случайных лесов), выходы которых являются входами нейронной сети. Обучение данной модели предлагается производить в два этапа. На первом этапе обучающая выборка делится на две части и одна из частей используется для обучения нескольких случайных лесов. На втором этапе используется другая часть выборки для итерационного обучения нейронной сети и улучшения качества работы случайных лесов. На каждой итерации результат работы модели на данных позволяет вычислить градиент дифференцируемой функции потерь по параметрам нейронной сети, а также по выходам случайного леса. Полученные градиенты используются для перерасчёта параметров нейронной сети методом градиентного спуска, а также для обучения дополнительного дерева в каждом случайном лесе аналогично методу градиентного бустинга.
Для доказательства эффективности данных методов проведены несколько экспериментов на различных выборках. Качество работы обоих предложенных методов сравнивалось с известными алгоритмами, использующих агрегирование один раз: случайным лесом и градиентным бустингом. Результаты экспериментов показали, что предложенные методы позволяют увеличить среднее значение ROC AUC на 4–5 сотых в сравнении с указанными алгоритмами на реальных задачах.
