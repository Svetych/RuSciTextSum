# -*- coding: utf-8 -*-
"""Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXr51lAshWxbrrQYNr0JgFsTLnD-T_kL
"""

import torch
from tqdm import tqdm
import torchvision

from transformers import (
    T5ForConditionalGeneration, T5TokenizerFast, T5Tokenizer,
    DataCollatorForLanguageModeling, TrainingArguments, Trainer,)
from datasets import load_dataset
from rouge import Rouge

path = '/content/drive/MyDrive/Colab Notebooks'
saved_model_path = path + '/archive'
checkpoint_path = path + '/t5-model-small'
model_name = "cointegrated/rut5-small"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def make_dataset_(data, tokenizer, max_length_text=2890, max_length_ref=200):
    dataset = []
    for inst in data:
        txt = tokenizer(inst['text'], add_special_tokens=True, max_length=max_length_text, padding="max_length", truncation=True)
        sum_ = tokenizer(inst['summary'], add_special_tokens=True, max_length=max_length_ref, padding="max_length", truncation=True).input_ids
        txt["labels"] = sum_
        dataset.append(txt)
    return dataset

def save(path, model, optimizer):
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, path)

def tests_res(data, model, tokenizer, max_length_text=2890):
    res, ref = [], []
    for inst in data:
        inp = tokenizer(inst['text'], add_special_tokens=True, max_length=max_length_text, padding="max_length", truncation=True, return_tensors='pt').to(device) 
        gen_ref = tokenizer.decode(model.generate(input_ids=inp.input_ids, attention_mask=inp.attention_mask)[0], skip_special_tokens=True)
        res.append(gen_ref)
        ref.append(inst['summary'])
    return res, ref

# Загрузка модели
tokenizer = T5Tokenizer.from_pretrained(model_name)
model_t5 = T5ForConditionalGeneration.from_pretrained(model_name)
optimizer = torch.optim.AdamW(model_t5.parameters(),lr=1e-4)

checkpoint = torch.load(saved_model_path + '/model_t5_small_4.pth', map_location='cpu')
model_t5.load_state_dict(checkpoint['model_state_dict'])
model_t5.to(device)
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Загрузка датасета
dataset_train = load_dataset('IlyaGusev/gazeta', revision="v2.0")["train"]
dataset_val = load_dataset('IlyaGusev/gazeta', revision="v2.0")["validation"]
dataset_test = load_dataset('IlyaGusev/gazeta', revision="v2.0")["test"]

print('Making train dataset')
dataset_train = make_dataset_(dataset_train, tokenizer)
print('Making val dataset')
dataset_val = make_dataset_(dataset_val, tokenizer)

# параметры для Тренера
training_args = TrainingArguments(
    output_dir= checkpoint_path,
    overwrite_output_dir=True,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=1,
    warmup_steps=10,
    gradient_accumulation_steps=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

'''
#if "Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`" occured:
! pip uninstall -y transformers accelerate
! pip install transformers accelerate
# then reload
'''

# Тренер
trainer = Trainer(
    model=model_t5,
    args=training_args,
    train_dataset=dataset_train,
    eval_dataset=dataset_val,
    tokenizer=tokenizer,
    optimizers = (optimizer, None)
)

# Обучение модели
model_t5.train()
trainer.train()
print('Saving model')
save(saved_model_path + '/model_t5_small_5.pth', model_t5, optimizer)

# Тестирование модели
model_t5.test()
print('Testing')
model_results, refs = tests_res(dataset_test, model_t5, tokenizer)
print('Done! Counting Rouge')
rouge = Rouge()
scores = rouge.get_scores(model_results, refs, avg=True)
print(scores)

