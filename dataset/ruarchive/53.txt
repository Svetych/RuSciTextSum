при адаптивном управлении и рекурсивной оценке параметров часто требуется рекурсивно корректировать оценку @xmath0 вектора @xmath1, который содержит постоянные, но неизвестные параметры @xmath2, используя измерения величины @xmath3 здесь @xmath4 - вектор известных данных, часто называемый регрессором, а @xmath5 - измерение сигнал об ошибке. цель настройки состоит в том, чтобы как ошибка оценки @xmath6, так и ошибка параметра @xmath7 были как можно меньше. существует несколько популярных методов решения вышеуказанной проблемы, например, метод наименьших квадратов. возможно, самый простой из них заключается в минимизации ошибки прогнозирования с помощью алгоритмов градиентного типа вида : @xmath8, где @xmath9 - постоянная, симметричная, положительно определенная матрица усиления. давайте определим @xmath10 и проанализируем дифференциальные уравнения и, которые в предположении, что @xmath11 тождественно равно нулю, читаются так: @xmath12 неотрицательная функция @xmath13 имеет производную по времени @xmath14, следовательно, @xmath15 проверка приведенного выше уравнения показывает, что @xmath16 ограничен во времени, таким образом, @xmath17, а также, что неотрицательная функция @xmath13 имеет производную по времени @xmath14. ошибка @xmath18 (нормы берутся на интервале @xmath19, где определены все сигналы ). это основные свойства, необходимые алгоритму для того, чтобы считаться подходящим кандидатом на роль тюнера в адаптивной системе управления. часто @xmath20 или что-то подобное также является желательным свойством. для получения последнего можно использовать нормализованные алгоритмы; однако относительные достоинства нормализованных и ненормализованных тюнеров все еще несколько спорны. другой альтернативой является использование изменяющегося во времени @xmath9, как это делается при настройке методом наименьших квадратов.    в [ sec : acceleration ] мы представляем настройщик, который устанавливает вторую производную от @xmath0, а в [ sec : ковариация ] сравнивается влияние белого шума @xmath5 на производительность двух алгоритмов. затем мы покажем несколько симуляций и сделаем заключительные замечания. классические настройки таковы, что скорость адаптации (первая производная параметров) устанавливается пропорциональной регрессору и ошибке прогнозирования @xmath21. мы предлагаем установить _ускорение _ параметров : @xmath22 обратите внимание, что приведенная выше формула реализуема (с использованием интеграторов @xmath23), если ошибка измерения отсутствует, поскольку неизвестный @xmath24 появляется только в скалярном произведении с @xmath25. выберите другую функцию ляпуновского вдохновения: @xmath26, беря производные по траекториям, дает @xmath27, интегрируя @xmath28, мы получаем @xmath29, что немедленно приводит к желаемым свойствам: @xmath30 свойство медленной вариации @xmath31 следует без необходимости нормализации, и теперь мы получаем @xmath32 вместо @xmath33, как раньше. мы могли бы рассматривать @xmath34 как модифицированную ошибку, которая может быть использована при анализе стабильности обнаруживаемой или `настраиваемой\" адаптивной системы с помощью аргумента ввода-вывода; см. @xcite. обобщением является @xmath35 с @xmath36 и @xmath37 постоянными, симметричными, положительно определенными матрицами @xmath38, такими что @xmath39 и @xmath40. свойства tuner, которые могут быть получены с использованием положительно определенной функции @xmath41 таким же образом, как и раньше, являются @xmath42 теперь мы рассмотрим влияние на ожидаемое значение и ковариацию @xmath43 наличия ошибки измерения. предположения заключаются в том, что @xmath11 - это белый шум с нулевым средним значением и ковариацией @xmath44 и что @xmath45 - это заданные детерминированные данные. для целей сравнения сначала рассмотрим, что происходит, когда применяется обычный тюнер при наличии ошибки измерения @xmath5 : @xmath46 решение приведенного выше уравнения может быть записано в терминах матрицы перехода состояний @xmath47 @xmath48 следующим образом @xmath49, следовательно, @xmath50, потому что @xmath51 по предположению. здесь обозначение @xmath52, обозначающее математическое ожидание относительно случайной величины @xmath5, используется для того, чтобы подчеркнуть, что стохастические свойства @xmath25 не рассматриваются. вывод состоит в том, что @xmath43 будет сходиться к нулю в среднем так же быстро, как и @xmath53. хорошо известная постоянность условий возбуждения на @xmath54 достаточна для того, чтобы произошло последнее.    чтобы изучить второй момент ошибки параметра, запишите @xmath55 ковариацию @xmath43 можно записать как сумму четырех слагаемых. первое является детерминированным. второй член @xmath56, потому что @xmath11 имеет нулевое среднее значение, и третий член также равен нулю. четвертый член @xmath57, где использовались теорема Фубини и факт @xmath58. выполнение интегрирования и добавление первого и четвертого членов приводит к @xmath59 этому уравнению можно дать следующую интерпретацию: для малого @xmath60, когда @xmath53 близко к тождеству, ковариация @xmath43 остается близкой к @xmath61, внешнему произведению ошибки в первоначальном предположении о значении параметры с самим собой. поскольку @xmath62, что произойдет, если @xmath54 будет постоянно возбуждать, @xmath63 стремится к @xmath64. это указывает на компромисс между более высокими скоростями сходимости и меньшей погрешностью установившегося параметра, которые требуют соответственно больших и меньших значений коэффициента усиления @xmath9. алгоритмы, которые стремятся к наилучшему из обоих миров сходимости параметров в среднеквадратичном смысле, часто используют изменяющийся во времени убывающий выигрыш; примером является алгоритм наименьших квадратов. теперь мы попытаемся провести аналогичный анализ для применяемого ускорителя, который приводит к дифференциальному уравнению @xmath65 пусть @xmath66, где @xmath67, @xmath68, каждый @xmath69 является функцией @xmath70, если не указано иное, а точка означает производную по отношению к первому аргументу. если @xmath71, @xmath72 следовать тем же рассуждениям, которые использовались для настройки скорости, можно сделать вывод, что @xmath73 и что @xmath74, однако, свойства настроек ускорения и скорости еще не сопоставимы напрямую, поскольку правая часть не поддается немедленной интеграции. чтобы получить сопоставимые результаты, мы используем неуклюжую, но легко проверяемую формулу @xmath75 ' \" \", действительную для произвольных скаляров @xmath76 и @xmath77, и делаем [ [ упрощающее предположение ] ] упрощающее предположение : + + + + + + + + + + + + + + + + + + + + + + + + для @xmath78 и 3, @xmath79, где @xmath80 - скаляры, а @xmath81 - единичная матрица @xmath82.    предварительное умножение с помощью @xmath83 $ ], последующее умножение с помощью @xmath83 ^\\top$ ], интегрирование от 0 до @xmath60 и использование упрощающего предположения дает формулу. @xmath84 ' \" \" принимая @xmath85, @xmath86 приводит к положительному полуопределенному результату, следовательно, @xmath87 комбинация и показывает, что @xmath88 может быть увеличена, не влияя на установившуюся ковариацию @xmath24. с другой стороны, чтобы уменьшить ковариацию, нам нужно увеличить @xmath89, что, грубо говоря, означает увеличение затухания в. поскольку значения @xmath88 и @xmath89 могут быть увеличены без ущерба для свойств стабильности, показанных в [ sec : acceleration ], с настройщиком ускорения может быть достигнут лучший компромисс в переходном режиме @xmath90 с установившейся производительностью, чем с настройщиком скорости, по крайней мере, в случае, когда @xmath91, @xmath92 и @xmath37 являются `скалярами. \" обратите внимание, что @xmath93 по конструкции. [ [ приблизительный анализ ] ] приблизительный анализ : + + + + + + + + + + + + + + + + + + + + + + вывод неравенства не требует каких-либо приближений и, следовательно, обеспечивает верхнюю границу для @xmath94, действительную независимо от @xmath54. менее консервативная оценка интеграла in может быть получена путем замены @xmath95 на его среднее значение @xmath96 в определении @xmath86 in. это приближение кажется разумным, поскольку @xmath86 появляется внутри интеграла, но требует более обширных имитационных исследований.    чтобы получить полезное неравенство, нам требуется @xmath97; а именно, используя дополнение Шура @xmath98 или, используя упрощающее предположение и заменяя @xmath95 его приближением @xmath96 @xmath99, предположим далее, что @xmath100. в поисках наименее консервативной оценки мы выбираем @xmath101, наименьшее значение из @xmath76, которое сохраняет @xmath97. таким образом, @xmath102 с @xmath103 \\bar{m}_1 \\left[\\begin{smallmatrix}{\\phi}^\\top_{11}(t,0 ) \\\\{ \\phi}^\\top_{12}(t,0 ) \\end{smallmatrix}\\right]}{4m_1 ^ 2 m_2m_3r(1+\\mu_2 ) -r}.$ ] используя @xmath104, мы повторяем предыдущий, точный результат. для больших положительных значений @xmath77 первый член правой части стремится к @xmath105, что указывает на то, что стационарная ковариация ошибки параметра уменьшается, когда сигнал @xmath25 увеличивается по величине, и что его можно уменьшить с помощью соответствующего выбора коэффициентов усиления @xmath88 и @xmath99. @xmath106. следовательно, ситуация для ускоряющего тюнера гораздо более благоприятна, чем для обычного. в моделировании в этом разделе сравнивается поведение ускоряющего тюнера с поведением градиентного тюнера и нормализованного градиентного тюнера. все моделирование проводилось в разомкнутом контуре, с использованием регрессора в виде двумерного сигнала и без измерительного шума. на рисунке [ рис. : шаг ] показаны значения @xmath107 и @xmath108 соответственно, когда @xmath25 является двумерным сигналом шага. на рисунке [ fig : sin ] регрессор представляет собой синусоиду, на рисунке [ fig : sia ] экспоненциально увеличивающуюся синусоиду, а на рисунке [ fig : prb ] псевдослучайный сигнал, сгенерированный с помощью matlab. не было предпринято никаких усилий для оптимизации выбора матриц усиления (@xmath91, @xmath92 и @xmath37 были выбраны равными идентичным ), и влияние шума измерения не учитывалось. производительность ускоряющего тюнера сопоставима, а иногда и превосходит производительность других тюнеров. = 2,5 дюйма = 2,5 дюйма = 2,5 дюйма = 2,5 дюйма = 2,5 дюйма = 2,5 дюйма = 2,5 дюйма = 2,5 дюйма другие идеи, связанные с настоящей, заключаются в замене интегратора на положительный - реальная передаточная функция @xcite и использование настройки высокого порядка ( @xcite ). настройка высокого порядка генерирует как выходные данные @xmath0, так и его производные вплоть до заданного порядка (в этом смысле мы могли бырассматриваем настоящий алгоритм как тюнер второго порядка ), но в отличие от ускоряющего тюнера требует производных от @xmath25 до того же порядка. мы ожидаем, что ускоряющие тюнеры найдут применение в адаптивном управлении нелинейными системами и, возможно, в решении проблемы топологической несовместимости, известной в литературе по адаптивному управлению как `проблема потери устойчивости\". стохастический анализ в [ sec : ковариация ] указывает на то, что характеристики производительности и сходимости ускоряющего тюнера, наряду с его умеренной вычислительной сложностью, действительно могут сделать его желательным инструментом для приложений адаптивной фильтрации. похоже, что лучший компромисс в переходном режиме @xmath90 с установившейся производительностью достижим с помощью ускоряющего тюнера, чем с помощью скоростного тюнера. чтобы проверить эту гипотезу, необходимо изучить свойства сходимости ускоряющего тюнера и их связь с постоянством условий возбуждения, а также провести более обширное моделирование при наличии измерительного шума.

мы предлагаем тюнер, подходящий для адаптивного управления и (в его версии с дискретным временем) приложений адаптивной фильтрации, который устанавливает вторую производную оценок параметров, а не первую производную, как это делается в подавляющем большинстве литературы. представлены сравнительные анализы стабильности и производительности. * ключевые слова: * адаптивное управление ; оценка параметров ; адаптивная фильтрация ; ковариационный анализ.