теория множеств была предложена с целью использования в областях классификации паттернов и обработки информации [1 ]. действительно, она привлекла многих исследователей, и их приложения к реальным задачам имеют большое значение. симпсон [ 2 ] представил нечеткую нейронную сеть min max (fmm), которая принимает мягкие решения об организации гипербоксов по степени их принадлежности к определенному классу, которая известна как функция принадлежности. гипербокс - это выпуклый прямоугольник, полностью представленный точками min и max. результаты классификации fmm полностью характеризуются с помощью функции принадлежности. наряду с этим элегантным предложением в [ 2] также представлены характеристики хорошего классификатора, среди которых нелинейная разделимость, перекрывающиеся классы и параметры настройки, которые, как оказалось, представляют большой интерес для исследовательского сообщества. симпсон также представил подход к кластеризации с использованием fmm в [3 ]. но многие проблемы в реальной жизни требуют как классификации, так и кластеризации. для решения этой проблемы gfmm [ 4 ] привнес эту общность. помимо общности, более существенным вкладом оказалась модификация функции принадлежности. представленная функция принадлежности вычисляет принадлежность к гипербоксу таким образом, что значение принадлежности равномерно уменьшается по мере удаления от гипербокса. другим слабым местом fmm были шаблоны, принадлежащие перекрывающейся области, где частота ошибочной классификации значительно высока. параметр настройки theta ( @xmath0 ), который управляет размером гипербокса, оказывает большое влияние на эту перекрывающуюся область. меньшие значения тэта приводят к меньшему количеству совпадений, обеспечивая высокую точность обучения, но эффективность сети снижается, а при больших значениях тэта точность снижается. для решения этой проблемы было представлено несколько подходов. ранее использовался процесс сжатия [1][4 ], который использовался для устранения всех перекрывающихся областей. у этого метода была внутренняя проблема представления шаблонов, не принадлежащих ни одному из гипербоксов, что, в свою очередь, снижало точность. сеть нечеткой классификации исключений/включений (hefc) была введена в [ 5 ], что еще больше сократило количество гипербоксов и повысило точность. гипербоксы включения использовались для представления шаблонов, принадлежащих к одному классу, в то время как гипербоксы исключения использовались для обозначения перекрывающейся области, рассматриваемой так, как если бы это был гипербокс. это понятие используется практически во всех недавно введенных моделях [ 6][7][8][9 ]. классификатор нечетких минимально-максимальных нейронных сетей с компенсаторными нейронами (fmcn) был представлен в [7]. авторы разделили перекрытие на три части, а именно: полное сдерживание, частичное перекрытие и отсутствие перекрытия, а затем новая функция принадлежности для учета принадлежности на основе значения компенсации. авторы также проанализировали, что аккуратная обработка перекрывающейся области автоматически приводит к нечувствительности к параметру размера гипербокса, @xmath0. основанная на ядре данных нечеткая нейронная сеть min - max (dcfmn) [ 8 ], дополнительно улучшенная по сравнению с fmcn. авторы устранили необходимость в категоризации с перекрытием. они также предлагают новую функцию принадлежности, основанную на шуме, геометрическом центре и ядрах данных гипербокса. несмотря на то, что dcfmn улучшил точность в нескольких случаях, есть несколько серьезных недостатков. * * dcfmn вводит две новые переменные, управляемые пользователем, @xmath1 и @xmath2. @xmath1 используется для подавления влияния шума, а @xmath2 используется для управления скоростью убывания функции принадлежности. эти две переменные сильно влияют на производительность модели, и, естественно, определение их значений является утомительной работой. * существует основополагающее предположение, что шум во всех гипербоксах одинаковый, что может быть неверно. более того, последовательность обучающих примеров также играет определенную роль. * mlf показывает, что эта функция принадлежности не всегда предпочтительна, поскольку она плохо работает при высоком проценте выборок, принадлежащих перекрывающейся области. многоуровневая нечеткая нейронная сеть min max (mlf) [ 9 ] решает проблему перекрывающейся области с помощью элегантного подхода. он использует отдельные уровни для перекрывающихся областей и монотонно уменьшает размер гипербокса ( @xmath0). в большинстве случаев mlf обеспечивает 100% точность обучения. хотя mlf достигает значительного рубежа, точность развлекательного тестирования гораздо важнее точности обучения, поскольку это значительно влияет на использование алгоритма в практических сценариях. в этом кратком изложении мы выявляем и определяем новую пограничную область, где частота ошибочной классификации существенна. насколько нам известно, подобный подход представлен впервые, по крайней мере, мы не сталкивались ни с одной подобной опубликованной работой. следовательно, мы предлагаем метод, основанный на центроидах данных, чтобы наглядно доказать, что обработка этой недавно появившейся области путаницы между гипербоксами разных классов значительно повышает точность тестирования.    статья организована следующим образом. mlf рассматривается в разделе ii. мы представили алгоритм d - mlf в разделе iii. наглядный пример и сравнительные результаты d - mlf с моделью mlf представлены в разделах iv и v соответственно. наконец, заключение приведено в разделе vi. многоуровневая нечеткая нейронная сеть min max (mlf) - это классификатор, который эффективно устраняет неправильную классификацию паттернов, принадлежащих перекрывающейся области, поддерживая древовидную структуру, которая представляет собой однородное дерево [9 ].    на этапе обучения mlf образцы непрерывно повторяются для формирования гипербоксов и перекрытий, каждая рекурсия приводит к одному уровню. эта рекурсивная процедура выполняется до достижения заданной максимальной глубины или до тех пор, пока не возникнет перекрытие. расширение hyperbox, основанное на управляющем параметре размера hyperbox ( @xmath0 ), проверяется с помощью уравнения ( 1), а расширение выполняется с помощью уравнения ( 2). @xmath3 @xmath4, где @xmath5 и @xmath6 - минимальная точка и максимальная точка гипербокса _ b _ соответственно, @xmath7 - размерность @xmath8 шаблона _ a _, а _ d _ - количество измерений. кроме того, перед каждой рекурсией @xmath0 обновляется с использованием уравнения ( 3 ) @xmath9, где @xmath10 и @xmath11 значения для следующего уровня и предыдущего уровня соответственно, а @xmath12, являющийся значением от 0 до 1, гарантирует, что размер гипербокса в перекрывающейся области меньше его предыдущего уровня.    на этапе тестирования сначала рекурсивно пересекаются области перекрытия, чтобы обнаружить соответствующую подсеть, к которой принадлежит тестовый шаблон. далее, на этом уровне класс гипербокса, имеющий наибольшее значение принадлежности к гипербоксам в обнаруженной подсети, выбирается в качестве прогнозируемого класса. mlf позволяет достичь более высоких показателей точности, чем предыдущие методы fmm. это связано с элегантным обращением с пограничной областью - областью путаницы. но после обучения остается место для еще одной границы. область, где функция принадлежности генерирует очень близкие по значениям значения, становится трудно присвоить классу с высокой степенью уверенности. согласно нашим экспериментам, mlf и все предыдущие классификаторы не очень хорошо работают в этой области. следовательно, предлагается определение этой новой области и методология ее решения. в этом разделе мы подробно рассказываем о недавно предложенном алгоритме, в частности, мы определяем новую граничную область, сгенерированную благодаря обученной сети, и предлагаем решение для правильной классификации тестовых шаблонов, принадлежащих к ней. _ рисунок 1 _ описывает структуру d - mlf, каждый узел в s@xmath13net содержит два сегмента, сегмент гипербоксов (hbs ) и перекрывающийся сегмент (ols). hbs представляет гипербоксы, сгенерированные на этом уровне, тогда как ols представляет перекрытия на этом уровне. наряду с информацией о гипербоксе, центр тяжести данных (dc). _ на рисунке 2 _ показана область путаницы, рассмотренная mlf и d - mlf. мы вводим граничную область, которая существует между любыми двумя гипербоксами, где, согласно нашим экспериментам, скорость неправильной классификации сравнительно высока. в предлагаемом методе рекомендации mlf остаются неизменными, в дополнение к этому мы используем расстояние с центроидами данных для улучшения скорости классификации в новой пограничной области. аналогично процедуре обучения mlf, d - mlf поддерживает @xmath14, используя структуры hbs и ols. сначала передаются все шаблоны, в результате чего создаются и расширяются гипербоксы с использованием уравнений (1 ) и ( 2). затем каждый гипербокс проверяется с остальными гипербоксами, чтобы обнаружить перекрытие, используя уравнение (4). @xmath15, где @xmath16 и @xmath17 - максимальные точки, а @xmath18 и @xmath19 - минимальные точки двух гипербоксов, между которыми проверяется перекрытие. более того, d - mlf добавляет новый шаг на этапе обучения, известный как вычисление центроида данных (dc), при котором dc всех входных шаблонов, принадлежащих каждому гипербоксу, сохраняется в hbs. dc вычисляется следующим образом: @xmath20, где @xmath21 - центр тяжести данных гипербокса @xmath22, @xmath23 - количество шаблонов, принадлежащих гипербоксу @xmath22, а @xmath24 - шаблон @xmath8 в гипербоксе @xmath22. если существует перекрытие, шаблоны, принадлежащие перекрываемой области, снова отправляются в процедуру обучения, где происходит создание hbs и ols для следующего уровня. этот процесс рекурсии выполняется позже для обучения всех шаблонов. из-за вычисления ols и процесса поиска шаблонов, принадлежащих ols, d - mlf и mlff не являются алгоритмами с одним проходом. в общем, учитывая n совпадений на первом уровне, все обучающие данные должны быть пройдены n раз. после этого, на последующих этапах, данные, принадлежащие перекрывающейся области, обрабатываются в порядке величины количества перекрытий в этой области. это новое открытие, противоречащее тому, что упоминали авторы mlf [9]. Обратите внимание, что шаблоны, принадлежащие перекрывающейся области, не являются частью вычисления dc. этот шаг гарантирует, что обучающие шаблоны, голосующие за более чем один класс, будут опущены при принятии окончательного решения. + net = d - mlf - train(net, @xmath0 ) + @xmath25 @xmath26 = h.centroid / h.membercount возвращает null h.centroid + = sample ; h.membercount + = 1 ; создать новый гипербокс h ; h.centroid = sample ; h.membercount = 1 ; sdata = образцы, которые находятся в i регионе ; hi.centroid -= s ; hi.membercount -= 1 ; создать блок перекрытия как @xmath27 и добавить в ols @xmath28 = d - mlf - train ( sdata, @xmath29 ) ; связать @xmath27 с @xmath28 с помощью ссылки @xmath30 ; @xmath31 исходный mlf использовал процесс принятия решений, основанный на решении подсетей. выбранная подсеть не обязательно должна быть конечным узлом в дереве. мы не изменяем эту модель, скорее улучшаем процесс того, как подсеть отмечает выбор. функция принадлежности, упомянутая в уравнении (11), используется для перекрывающихся блоков. после рекурсивного обхода ols обнаруживается подходящая подсеть, к которой принадлежит тестовый шаблон. на этот раз для вычисления принадлежности к гипербоксам внутри выбранной подсети используется функция принадлежности, описанная в уравнении (6). @xmath32 \\\\ [ 1- f(v_i^j - a_h^i,\\gamma_i ) ] ) ) \\\\ f(x,\\gamma)=\\begin{случаи}{1}\\;\\;\\;\\ ; если \\ ; x\\gamma\\;\u003e\\;1 \\\\ { x}\\;\\;\\;\\; если \\ ; 0\\;\\leq\\;x\\gamma\\;\\leq\\;1 \\\\ { 0}\\;\\;\\;\\ ; если \\ ; x\\gamma\\;\u003c\\;0 \\end{cases } \\end{split}\\ ] ] где @xmath33 представляет принадлежность образца @xmath34 к гипербоксу @xmath35. @xmath36 - это разница между минимальной и максимальной точками с выборкой @xmath34, а @xmath37 - это параметр настройки для управления нечеткостью.    в пределах этих значений принадлежности для определения границы выбираются гипербоксы с двумя самыми высокими значениями. средняя область этих гипербоксов, управляемая @xmath38, рассматривается как граничная область. @xmath38 - это управляемая пользователем переменная, указанная в процентном значении. на этом этапе необходимо проверить, принадлежит ли тестовый шаблон граничной области. мы определяем @xmath391 и @xmath392 как углы падения между тестовым шаблоном и двумя гипербоксами соответственно. значение включения оценивается следующим образом : @xmath40 далее, на основе значения включения, выбирается выходной класс. если шаблон существует в области за пределами определенной границы, мы просто следуем по пути mlf и классифицируем шаблон на основе максимального значения принадлежности, которое уже вычислено. если шаблон принадлежит граничной области, вычисляется евклидово расстояние [ 10 ] между тестовым шаблоном и центроидами данных выбранных гипербоксов. следовательно, в зависимости от значения включения выходные данные сети обозначаются либо как класс максимального значения @xmath41 среди всех гипербоксов, либо как минимальное значение расстояний между двумя самыми верхними гипербоксами @xmath42, где @xmath43 задается через ; @xmath44, где @xmath45 - принадлежность к классу @xmath8 для тестового образца в подсети @xmath46 @xmath47 является границей между подсетью @xmath46 и соответствующим полем перекрытия, которое включает подсеть, если тестовый образец находится в этом поле перекрытия. и @xmath48 - это выходные данные ols, которые задаются уравнением ( 10 ) @xmath49, где @xmath50 - количество блоков перекрытия в ols, а @xmath51 - функция принадлежности блока перекрытия @xmath35 для тестового образца @xmath34, заданного уравнением ( 11 ) @xmath52 и @xmath53, задается по уравнению(12 ) @xmath54, где @xmath55 - евклидово расстояние, вычисленное между выборкой @xmath46 и центром тяжести данных самого верхнего гипербокса @xmath8, используя уравнение (13 ) @xmath56 out = d - mlf - тест (сеть, выборка ) + @xmath25 out = d - mlf - тест ( @xmath28, sample ) ; возвращает null ; mv = [ ] ; mv + = членство (sample, @xmath57 ) ; = [ max(mv ), max(mv ( mv @xmath58 max(mv ) ) ) ] d = eudistance(sample, h1.dc, h2.dc ) ; out = min(d).class ; out = max (mv).class ; @xmath31 на этой иллюстрации мы описываем эффективность предлагаемой модели, четко указывая на идентификацию и обработку заявленной области путаницы. _ рисунок 3 _ иллюстрирует двухдиапазонное пространство данных. мы рассматриваем 14 выборок данных для обучения и 6 выборок данных для тестирования. параметр размера гипербокса ( @xmath0 ) зафиксирован на уровне 0,3, а параметр границы ( @xmath38 ) зафиксирован на уровне 5%. как mlf, так и d - mlf создают два гипербокса на уровне @xmath59. d - mlf также вычисляет центроиды данных (dc) для каждого гипербокса, @xmath60 и @xmath61. здесь центроидами данных @xmath60 и @xmath61 являются @xmath62 и @xmath63 соответственно. шаблоны, которые не принадлежат пограничной области, корректно классифицируются mlf. но когда дело доходит до пограничной области, он не может правильно классифицировать шаблоны. в то время как предлагаемый d - mlf также лучше работает в пограничной области, поскольку его принятие решений не полностью основано на значении принадлежности, но также учитывает центроиды данных. можно отметить, что закономерности в приведенном выше примере распределены неравномерно. что является очень распространенным сценарием в примерах реального мира. это происходит из-за преобладания таких параметров, как выбросы, временная природа переменных и т.д. из-за них в большинстве случаев шаблоны в общих данных, а в случае нечеткой иерархии min max внутри гипербоксов, не будут равномерно распределены по всем измерениям. как было продемонстрировано выше, предлагаемый нами метод обрабатывает их элегантно, без многих модификаций, соответствующих уровню техники. эффективность предлагаемого метода (d - mlf) изучается на основе скорости классификации. были проведены различные эксперименты для тестирования d - mlf на различных стандартных наборах данных. были использованы стандартные наборы данных, такие как iris, glass, wine, wisconsin breast cancer (wbc), wisconsin diagnostic breast cancer (wdbc) и ионосфера. эти наборы данных были получены из хранилища баз данных машинного обучения uci [11 ]. в этих экспериментах параметр размера гипербокса ( @xmath0 ) был выбран равным 0,2, 0,5 и 0,9. это было сделано для выполнения измерений по всему спектру. по мере увеличения размера гипербокса увеличивается количество совпадений, а также частота неправильной классификации. мы равномерно распределяем данные для обучения и тестирования. приведены средние результаты по 100 экспериментам. для каждой итерации данные обучения и тестирования выбираются случайным образом. _ таблица 1 _ показывает результаты, мы сравниваем наши результаты с методом mlf, поскольку уже доказано, что он работает лучше, чем ранее предложенные методы fmm [ 9 ]..результаты [ cols=\"^,^,^,^\",options=\"заголовок \", ] в этом кратком изложении мы внедрен новый метод классификации mlf на основе граничной области и расстояния для обработки шаблонов, принадлежащих этой граничной области. метод, основанный на центроиде данных, d - mlf, сводит к минимуму значимость выбросов и аналогичных ошибок при принятии решений. было убедительно доказано, что данное предложение превосходит все ранее предложенные методы fmm. что еще более важно, мы предложили модель, подходящую для работы с данными в реальном мире, расширяющую уровень техники. 

недавно была предложена многоуровневая нечеткая нейронная сеть min max (mlf), которая повышает точность классификации за счет обработки перекрывающейся области (области путаницы) с помощью древовидной структуры. в этом кратком изложении предлагается расширение mlf, которое определяет новую пограничную область, где ранее предложенные методы оценивают решения с меньшей достоверностью и, следовательно, ошибочная классификация встречается чаще. представлена методология для более точной классификации шаблонов. наша работа улучшает процедуру тестирования с помощью центроидов данных. мы приводим наглядный пример, четко подчеркивающий преимущество нашего подхода. также представлены результаты по стандартным наборам данных, которые наглядно доказывают постоянное улучшение скорости классификации. гипербокс, нечеткий минимум - максимум, центроиды данных, нейронные сети, нейрофуззи, классификация, машинное обучение.