методы выбора переменных, основанные на теории штрафов, привлекли большое внимание при анализе многомерных данных. принципиальный подход обусловлен lasso из @xcite, который использует штраф @xmath0-norm. @xcite также указал, что оценку лассо можно рассматривать как способ апостериорного распределения. действительно, штраф @xmath1 может быть преобразован в априор Лапласа. более того, этот априор может быть выражен в виде смеси гауссовых шкал. таким образом, это привело к байесовским разработкам лассо и его вариантов @xcite. также проводилась работа по невыпуклому наказанию в рамках параметрической байесовской структуры. @ xcite вывели свой алгоритм локальной линейной аппроксимации (lla), объединив алгоритм максимизации математического ожидания (em) с обратным преобразованием Лапласа. в частности, они показали, что штраф @xmath2 с @xmath3 может быть получен путем смешивания распределения Лапласа со стабильной плотностью. другие авторы показали, что априорное значение, вызванное штрафом, называемым невыпуклым логарифмическим штрафом и определенным в уравнении ( [ eqn : logp ] ) ниже, интерпретируется как масштабная смесь распределений Лапласа с обратным распределением гамма-смешивания @ xcite. недавно @ xcite расширил этот класс дисперсионных смесей Лапласа, используя обобщенное обратное гауссовское распределение смешивания. связанные методы включают байесовское гипер - лассо @xcite, модель подковы @ xcite и априор Дирихле Лапласа @xcite.    параллельно непараметрические байесовские подходы были применены к выбору переменной @xcite. например, в бесконечной гамма-модели Пуассона @xcite отрицательные биномиальные процессы используются для описания неотрицательных целочисленных матриц, что приводит к непараметрическому байесовскому подходу к выбору признаков в условиях неконтролируемого обучения. процесс бета - Бернулли предоставляет непараметрический байесовский инструмент для моделирования разреженности @xcite. кроме того, @xcite предложил непараметрический подход для смесей с нормальной дисперсией и показал, что этот подход тесно связан с процессами lvy. позже @ xcite построил разреженные априорные значения, используя приращения субординаторов, которые встраивают конечномерные смеси нормальных дисперсий в бесконечные. таким образом, это обеспечивает новую структуру для построения априорных значений, вызывающих разреженность. в частности, @xcite обсудил использование @xmath4-стабильных субординаторов и инвертированных бета-субординаторов для моделирования совместных априорных значений коэффициентов регрессии. @xcite установил связь двух невыпуклых штрафных функций, которые называются log и exp и определены в уравнениях ( [ eqn : logp ] ) и ( [ eqn : exp ] ) ниже, с преобразованиями Лапласа гамма- и пуассоновских субординаторов. субординатор - это одномерный процесс lvy, который почти наверняка неубывает @ xcite.    в этой статье мы дополнительно изучаем применение субординаторов в байесовских невыпуклых задачах наложения штрафов в сценариях контролируемого обучения. в отличие от предыдущих методов обработки, мы моделируем параметры скрытой усадки, используя субординаторы, которые определяются как стохастические процессы регуляризации параметров. в частности, мы рассматриваем два семейства составных пуассоновских субординаторов: непрерывные составные пуассоновские субординаторы, основанные на гамма-случайной величине @ xcite, и дискретные составные пуассоновские субординаторы, основанные на логарифмической случайной величине @xcite. соответствующие меры lvy являются обобщенными мерами gamma @ xcite и пуассона соответственно. мы показываем, что как гамма, так и пуассоновские субординаторы являются предельными случаями этих двух семейств составных пуассоновских субординаторов. поскольку показатель Лапласа субординатора является функцией Бернштейна, у нас есть два семейства невыпуклых штрафных функций, предельными случаями которых являются невыпуклые log и exp. Кроме того, эти два семейства невыпуклых штрафных функций могут быть определены с помощью композиции log и exp, в то время как непрерывный и дискретный составные пуассоновские субординаторы являются смесями гамма- и пуассоновских процессов. напомним, что параметр скрытой усадки является стохастическим процессом параметра регуляризации. мы формулируем иерархическую модель с несколькими параметрами регуляризации, что приводит к байесовскому подходу к невыпуклому наказанию. чтобы снизить вычислительные затраты, мы разрабатываем алгоритм ecme (\"для математического ожидания / условной максимизации\" ) @ xcite, который может адаптивно корректировать параметры локальной регуляризации при одновременном поиске разреженного решения. остальная часть статьи организована следующим образом. в разделе [ sec : levy ] рассматривается использование процессов lvy в байесовских задачах разреженного обучения. в разделе [ sec : gps ] мы изучаем два семейства сложных пуассоновских процессов. в разделе [ sec : blrm ] мы применяем процессы lvy к байесовской линейной регрессии и разрабатываем алгоритм ecme для нахождения разреженного решения. мы проводим эмпирические оценки с использованием смоделированных данных в разделе [ sec : эксперимент] и завершаем нашу работу в разделе [ sec: заключение]. наша работа основана на понятии Бернштейна и полностью монотонных функциях, а также субординаторах. пусть @xmath5 с @xmath6. говорят, что функция @xmath7 полностью монотонна, если @xmath8 для всех @xmath9 и Бернштейна, если @xmath10 для всех @xmath9.    грубо говоря, субординатор - это одномерный процесс lvy, который почти наверняка неубывает. наша работа в основном мотивирована свойством субординаторов, приведенным в лемме [ lem : subord ] @xcite. [ lem : subord ] если @xmath11 является субординатором, то преобразование Лапласа его плотности принимает форму @xmath12, где @xmath13 - это плотность @xmath14, а @xmath15, определенная в @xmath16, называется _ показателем лапласа _ субординатора и имеет следующее представление @xmath17 \\nu ( d u).\\ ] ] здесь @xmath18 и @xmath19 - это мера lvy, такая, что @xmath20. и наоборот, если @xmath15 - это произвольное отображение из @xmath21, заданное выражением ( [ eqn : psi ] ), то @xmath22 - это преобразование Лапласа плотности субординатора. хорошо известно, что показатель лапласа @xmath15 равен Бернштейну, а соответствующее преобразование Лапласа @xmath23 полностью монотонно для любого @xmath24 @xcite. более того, любая функция @xmath25 с @xmath26 является функцией Бернштейна тогда и только тогда, когда она имеет представление, как в выражении ( [ eqn : psi ] ). очевидно, что @xmath15, как определено в выражении ( [ eqn : psi ] ), удовлетворяет @xmath27. в результате @xmath15 неотрицателен, не уменьшается и вогнут в @xmath16. нам дан набор обучающих данных @xmath28, где @xmath29 - входные векторы, а @xmath30 - соответствующие выходные данные. теперь мы обсудим следующую модель линейной регрессии: @xmath31, где @xmath32, @xmath33^t$ ] и @xmath34 - гауссовский вектор ошибок @xmath35. мы стремимся найти разреженную оценку вектора коэффициентов регрессии @xmath36, используя байесовский невыпуклый подход.    в частности, мы рассматриваем следующую иерархическую модель для коэффициентов регрессии @xmath37 s: @xmath38 & \\stackrel{iid}{\\sim } p(\\eta_j ), \\\\\\sigma & \\sim\\iga(\\alpha_{\\sigma}/2, \\beta_{\\sigma}/2),\\end{выровнено}\\ ] ] где параметры @xmath39 называются параметрами скрытой усадки, а обратная гамма-априорная имеет следующую параметризацию: @xmath40 кроме того, мы рассматриваем @xmath39 как @xmath41, то есть @xmath42. здесь @xmath43 определен как субординатор. пусть @xmath44, определенный в @xmath16, является показателем лапласа субординатора. используя @xmath45, можно показать, что @xmath46 определяет невыпуклую штрафную функцию @xmath47 для @xmath48. более того, @xmath46 недифференцируем в начале координат, потому что @xmath49 и @xmath50. таким образом, он способен индуцировать разреженность. в связи с этим @xmath51 формирует априор для @xmath47. из леммы [ lem : subord ] следует, что априор может быть определен с помощью преобразования Лапласа. подводя итог, мы имеем следующую теорему. [ thm : lapexp00 ] пусть @xmath15 - ненулевая функция Бернштейна в @xmath16. если @xmath52, то @xmath46 - недифференцируемая и невыпуклая функция @xmath47 в @xmath53. кроме того, @xmath54, где @xmath43 является некоторым подчиненным. напомним, что @xmath14 определен как параметр скрытой усадки @xmath55, и в разделе [ sec : blrm ] мы увидим, что @xmath56 играет ту же роль, что и параметр регуляризации (или гиперпараметр настройки). таким образом, существует важная связь между параметром скрытой усадки и соответствующим параметром регуляризации, то есть @xmath57. поскольку @xmath58, каждый параметр скрытой усадки @xmath39 соответствует локальному параметру регуляризации @xmath59. следовательно, у нас есть непараметрическая байесовская формулировка для параметров скрытой усадки @xmath39 s. Также стоит отметить, что @xmath60, где @xmath61 обозначает распределение Лапласа с плотностью, заданной @xmath62, тогда @xmath63 определяет надлежащую плотность некоторой случайной величины (обозначается @xmath64). впоследствии мы получаем правильный предварительный @xmath65 для @xmath47. более того, это предварительное значение можно рассматривать как смесь по шкале Лапласа, т.е. смесь @xmath66 с распределением смешивания @xmath67. если @xmath68, то @xmath69 - неподходящая плотность. таким образом, @xmath70 также не подходит в качестве предшествующего @xmath47. однако мы по-прежнему рассматриваем @xmath70 как смесь @xmath66 с распределением смешивания @xmath67. в этом случае мы используем терминологию псевдоприоров для плотности, которая также используется @xcite.      очевидно, @xmath71 - это Бернштейн. это крайний случай, потому что у нас есть это @xmath72, @xmath73 и это @xmath74, где @xmath75 обозначает дельта-меру Дирака в @xmath56, которая соответствует детерминированному процессу @xmath76. мы можем исключить этот случай, предположив @xmath77 в выражении ( [ eqn : psi ] ), чтобы получить строго вогнутую функцию Бернштейна. фактически, мы можем наложить условие @xmath78. это, в свою очередь, приводит к @xmath77 из-за @xmath79. в этой статье мы используем показатели Лапласа в невыпуклых задачах штрафования. для этой цели мы будем рассматривать только субординатор без дрейфа, т.е., @xmath77. эквивалентно, мы всегда предполагаем, что @xmath80. здесь мы берем невыпуклый журнал и штрафы exp в качестве двух конкретных примеров (также смотрите * ? ? ? логарифмический штраф определяется @xmath81, в то время как exp-штраф задается @xmath82 очевидно, что эти две функции являются функциями Бернштейна на @xmath16. более того, они удовлетворяют @xmath27 и @xmath83. также непосредственно проверено, что @xmath84 \\nu(du ) }, \\ ] ] где мера lvy @xmath19 задана @xmath85, соответствующий субординатор @xmath86 является гамма-субординатором, потому что каждый @xmath14 следует гамма-распределению с параметрами @xmath87, с плотностью, заданной @xmath88, мы также обратите внимание, что соответствующий псевдоприоритет задается @xmath89 кроме того, если @xmath90, псевдоприоритет является правильным распределением, которое представляет собой смесь @xmath91 со смешанным распределением @xmath92.    что касается штрафа за exp, мера lvy равна @xmath93. поскольку @xmath94 d b } = \\infty,\\ ] ] тогда @xmath95 $ ] является неподходящим предшествующим @xmath47. кроме того, @xmath96 является подчинителем Пуассона. в частности, @xmath14 представляет собой распределение Пуассона с интенсивностью @xmath97, принимающей значения из набора @xmath98. то есть @xmath99, который мы обозначаем через @xmath100. в этом разделе мы исследуем применение составных пуассоновских субординаторов при построении невыпуклых штрафных функций. пусть @xmath101 - последовательность независимых и одинаково распределенных (i.i.d. ) вещественнозначных случайных величин с общим законом @xmath102, и пусть @xmath103 - пуассоновский процесс с интенсивностью @xmath104, который не зависит от всех @xmath105. тогда @xmath106 для @xmath24 следует составному распределению Пуассона с плотностью @xmath107 (обозначается @xmath108), и, следовательно, @xmath43 называется составным пуассоновским процессом. сложный пуассоновский процесс является субординатором тогда и только тогда, когда @xmath105 являются неотрицательными случайными величинами @xcite. стоит отметить, что если @xmath109 является подчинителем Пуассона, заданным в выражении ( [ eqn : possion ] ), это эквивалентно утверждению, что @xmath14 следует за @xmath110. в частности, мы изучаем два семейства неотрицательных случайных величин @xmath111: неотрицательные непрерывные случайные величины и неотрицательные дискретные случайные величины. соответственно, у нас есть непрерывные и дискретные составные пуассоновские субординаторы @xmath109. мы покажем, что и гамма-субординаторы, и пуассоновские субординаторы являются предельными случаями составных пуассоновских субординаторов.      в первом семействе @xmath111 является гамма-случайной величиной. в частности, пусть @xmath112 и @xmath111 являются идентификаторами из дистрибутива @xmath113, где @xmath114, @xmath115 и @xmath116. составной пуассоновский субординатор может быть записан следующим образом @xmath117 плотность субординатора тогда задается через @xmath118, мы обозначаем его через @xmath119. среднее значение и дисперсия равны @xmath120 соответственно. преобразование Лапласа задается @xmath121, где @xmath122 - функция Бернштейна вида @xmath123.\\ ] ] соответствующая мера lvy задается @xmath124 обратите внимание, что @xmath125 является гамма-мерой для случайной величины @xmath126. таким образом, мера lvy @xmath127 упоминается как обобщенная гамма-мера @xcite. функция Бернштейна @xmath128 была изучена @xcite для анализа выживаемости. однако мы рассматриваем ее применение в моделировании разреженности. ясно, что @xmath128 для @xmath114 и @xmath116 удовлетворяет условиям @xmath129 и @xmath130. кроме того, @xmath131 является неотрицательной и невыпуклой функцией @xmath47 на @xmath48, и это возрастающая функция @xmath132 на @xmath133. более того, @xmath131 непрерывен по отношению к @xmath47, но недифференцируем в начале координат. это подразумевает, что @xmath131 можно рассматривать как штраф, вызывающий разреженность. нас интересуют предельные случаи, которые @xmath134 и @xmath135. [ pro : first ] пусть @xmath136, @xmath128 и @xmath137 определяются выражениями ( [ eqn : first_tt ] ), ( [ eqn : first ] ) и ( [ eqn : first_nu ] ) соответственно. затем 1. @xmath138 и @xmath139; 2. @xmath140 и @xmath141 ; 3. @xmath142 и @xmath143. это утверждение может быть получено с помощью прямых алгебраических вычислений. утверждение [ pro : first ] говорит нам, что предельные случаи дают невыпуклые функции log и exp. более того, мы видим, что @xmath14 сходится в распределении к гамма-случайной величине с формой @xmath144 и масштабом @xmath145, как @xmath146, и к пуассоновской случайной величине со средним значением @xmath144, как @xmath147. хорошо известно, что @xmath148 вырождается в логарифмическую функцию @xcite. здесь мы показали, что @xmath122 приближается к exp как @xmath147. мы приводим другой специальный пример в таблице [ tab : exam ], когда @xmath149. мы ссылаемся на соответствующий штраф как на _ линейно-дробную _ (lfr) функцию. для простоты обозначения мы соответственно заменяем @xmath150 и @xmath151 на @xmath145 и @xmath152 в функции lfr. плотность субординатора для функции lfr задается @xmath153, таким образом, мы говорим, что каждый @xmath14 следует квадратичному процессу Бесселя без дрейфа @xcite, который представляет собой смесь дельта-меры Дирака и рандомизированного гамма-распределения @xcite. мы обозначаем плотность @xmath14 через @xmath154. lllll & функции Бернштейна & меры lvy @xmath137 & подчиненные @xmath14 & приоритеты + журнал & @xmath155 & @xmath156 & @xmath157 & правильный @xmath158 + exp & @xmath159 $ ] & @xmath160 & @xmath161 & неправильный + lfr & @xmath162 & @xmath163 & @xmath164 & неправильный + чел & @xmath165 $ ] & @xmath166 & @xmath167 & неправильный + + + во втором случае мы рассматриваем семейство дискретных составных пуассоновских субординаторов. в частности, @xmath111 является дискретным и принимает значения в @xmath168. и это определяется как логарифмическое распределение @xmath169, где @xmath170 и @xmath171, с функцией массы вероятности, заданной @xmath172 более того, мы позволяем @xmath173 иметь распределение Пуассона с интенсивностью @xmath174, где @xmath114. тогда @xmath14 распределяется в соответствии с отрицательным биномиальным (nb) распределением @xcite. функция массы вероятности @xmath14 задается @xmath175, которая обозначается как @xmath176. таким образом, мы говорим, что @xmath14 следует за подчиненным nb. пусть @xmath177 и @xmath178. можно проверить, что @xmath179 имеет то же среднее значение и дисперсию, что и распределение @xmath119. соответствующее преобразование Лапласа затем приводит к появлению нового семейства функций Бернштейна, которое задано @ xmath180.\\ ] ] мы называем это семейство функций Бернштейна функциями _ compound exp - log _ ( cel). производная первого порядка от @xmath181 w.r.t. @xmath182 задана @xmath183 мера lvy для @xmath181 задана @xmath184 доказательство приведено в приложении 1. мы называем эту меру lvy _ обобщенной мерой Пуассона _ относительно обобщенной гамма-меры.    как и @xmath128, @xmath181 может определять семейство невыпуклых штрафов, вызывающих разреженность. кроме того, @xmath181 для @xmath114, @xmath185 и @xmath116 удовлетворяет условиям @xmath186, @xmath187 и @xmath188. мы представляем специальную функцию cel @xmath189, а также соответствующие @xmath14 и @xmath137 в таблице [ tab : exam ], где мы заменяем @xmath190 и @xmath150 на @xmath152 и @xmath145 для простоты записи. теперь мы рассмотрим предельные случаи. [ pro:8 ] предположим, что @xmath137 определяется выражением ( [ eqn : second_nu ] ) для фиксированных @xmath185 и @xmath116. тогда у нас есть это 1. @xmath191 и @xmath192. @xmath193 и @xmath194. 3. @xmath195 и @xmath142. 4. @xmath196 и @xmath197 обратите внимание, что @xmath198 это показывает, что @xmath127 сходится к @xmath199, как @xmath200. аналогично, мы получаем вторую часть предложения [pro:8]-(d ), которая подразумевает, что, поскольку @xmath200, @xmath14 сходится в распределении к гамма-случайной величине с параметром формы @xmath144 и параметром масштаба @xmath145. альтернативное доказательство приведено в приложении 2. предложение [ pro:8 ] показывает, что @xmath181 вырождается в exp как @xmath147, а в log как @xmath200. это показывает интересную связь между @xmath128 в выражении ( [ eqn : first ] ) и @xmath181 в выражении ( [ eqn : second ] ); то есть они имеют одинаковое ограничивающее поведение. мы отмечаем, что для @xmath201, @xmath202\\ ] ], который является композицией функций log и exp, и что @xmath203\\ ] ], который является композицией функций exp и log. фактически, композиция любых двух функций Бернштейна по-прежнему является функцией Бернштейна. таким образом, композиция также является показателем Лапласа некоторого субординатора, который затем представляет собой смесь субординаторов, соответствующих исходным двум функциям Бернштейна @ xcite. это приводит нас к альтернативному выводу для субординаторов, соответствующих @xmath122 и @xmath204. то есть, у нас есть следующая теорема, доказательство которой приведено в приложении 3. [ thm : poigam ] подчинитель @xmath14, связанный с @xmath128, распределяется в соответствии со смесью распределений @xmath205 со смешиванием @xmath206, в то время как @xmath14, связанный с @xmath181, распределяется в соответствии со смесью @xmath207 дистрибутивы со смешиванием @xmath208.    кроме того, следующая теорема иллюстрирует ограничивающее свойство субординаторов, когда @xmath145 приближается к 0. [ thm : limit ] пусть @xmath209 будет фиксированной константой в @xmath210 $ ].    1. если @xmath211, где @xmath212 $ ] или @xmath213, то @xmath14 сходится по вероятности к @xmath56, как @xmath214. 2. если @xmath215, где @xmath216\\ ] ] или @xmath213, то @xmath14 сходится по вероятности к @xmath56, как @xmath214. доказательство приведено в приложении 4. поскольку @xmath14 сходится по вероятности к @xmath56 \" подразумевает, что @xmath14 сходится по распределению к @xmath56\", мы имеем, что @xmath217 наконец, рассмотрим четыре невыпуклые штрафные функции, приведенные в таблице [ вкладка : экзамен]. мы представляем следующее свойство. то есть, когда @xmath213 и для любого фиксированного @xmath116, мы имеем @xmath218 \\leq\\frac{s}{\\gamma s { + } 1 } \\leq\\frac{1}{\\gamma } [ 1 { - } \\exp ( { - } \\gamma s ) ] \\leq\\frac { 1}{\\gamma } \\log\\big({\\gamma } s { + } 1 \\big ) \\leq s,\\ ] ] с равенством только при @xmath219. доказательство приведено в приложении 5. это свойство также проиллюстрировано на рисунке [ рис. : penalty ]. в таблице [ вкладка : экзамен ] с @xmath220 и @xmath71. ] мы применяем составные пуассоновские субординаторы к байесовской задаче разреженного обучения, приведенной в разделе [ sec : levy ]. определяя @xmath221, мы переписываем иерархическое представление для совместного предшествующего @xmath37 в рамках регрессионной структуры. то есть, мы предполагаем, что @xmath222 & \\stackrel{ind}{\\sim } & l(b_j|0, \\sigma ( 2\\eta_j)^{-1 } ), \\\\ f_{t^{*}(t_j)}(\\eta_j ) & { \\propto } & \\eta_j ^{-1 } f_{t(t_j)}(\\eta_j),\\end{выровнено}\\ ] ], что подразумевает, что @xmath223 совместный маргинальный псевдоприор @ xmath37 s задается @xmath224 мы увидим в теореме [ thm : постер ], что полное условное распределение @xmath225 является правильным. таким образом, максимальная _ апостериорная _ (map) оценка @xmath226 основана на следующей задаче оптимизации: @xmath227 очевидно, что @xmath59 - это параметры локальной регуляризации, а @xmath228 - параметры скрытой усадки. более того, интересно, что @xmath43 (или @xmath55 ) определяется как подчиненный по отношению к @xmath56. полное условное распределение @xmath229 сопряжено с предыдущим, которым является @xmath230. в частности, это обратное гамма-распределение вида @xmath231.\\ ] ] в следующем эксперименте мы используем неправильный приоритет формы @xmath232 (т.е. @xmath233 ). Очевидно, что @xmath229 все еще является обратным гамма-распределением в этой настройке. кроме того, основываясь на @xmath234 \\prod_{j=1}^p \\exp(-\\frac { \\eta _ j}{\\sigma } |b_j|)\\vadjust{\\eject}\\ ] ] и доказательстве теоремы [ thm : плакат] (см. приложение 6), мы имеем, что условное распределение @xmath235 является правильным. однако абсолютные термины @xmath236 делают форму @xmath237 незнакомой. таким образом, алгоритм выборки Гиббса недоступен, и мы прибегаем к алгоритму em для оценки модели. обратите внимание, что если @xmath238 правильный, то соответствующая нормализующая константа задается @xmath239 d |b_j|= 2 \\int_{0}^{\\infty } \\exp\\big [ -t_j \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\big ] d ( |b_j|/\\sigma),\\ ] ] который не зависит от @xmath240. кроме того, условное распределение @xmath241 не зависит от нормализующего термина. в частности, у нас всегда есть то @xmath242, которое является правильным. как показано в таблице [ tab : exam ], за исключением log с @xmath243, который может быть преобразован в правильный приоритет, остальные функции Бернштейна не могут быть преобразованы в правильные приоритеты. в любом случае, наше апостериорное вычисление непосредственно основано на маргинальном псевдо - априорном @xmath244. мы игнорируем задействованный нормализующий член, потому что он бесконечен, если @xmath244 является неправильным, и он не зависит от @xmath240, если @xmath244 является правильным. учитывая @xmath245-ю оценку @xmath246 из @xmath247 на e - шаге алгоритма em, мы вычисляем @xmath248 p(\\eta_j|b_j ^{(k ) }, \\sigma ^{(k ) }, t_j ) } d \\eta_j + \\log p(\\sigma ) \\\\ & \\propto-\\frac{n+\\alpha_{\\sigma}}{2 } \\log\\sigma{- } \\frac{\\|{\\bf y } { -}{\\bf x}{\\bf b}\\|_2 ^ 2 + \\beta_{\\sigma}}{2 \\sigma } - ( p+1 ) \\log \\sigma \\\\ & \\quad- \\frac{1 } { \\sigma } \\sum_{j=1}^p здесь мы опускаем некоторые термины, которые не зависят от параметров @xmath240 и @xmath226. на самом деле, нам нужно только вычислить @xmath249 на e - шаге. учитывая, что @xmath250 и принимая производную от @xmath236 по обе стороны приведенного выше уравнения, мы имеем, что @xmath251 m - шаг максимизирует @xmath252 w.r.t.@xmath253. в частности, получается, что: @xmath254 приведенный выше алгоритм em связан с процедурой линейной локальной аппроксимации (lla) @xcite. более того, он обладает тем же свойством сходимости, что и @xcite и @xcite. подчинители помогают нам установить прямую связь между параметрами локальной регуляризации @xmath59 s и параметрами скрытой усадки @xmath39 s (или @xmath41). однако, когда мы реализуем оценку карты, возникает проблема с выбором этих параметров локальной регуляризации. мы используем алгоритм ecme (\"для математического ожидания / условной максимизации\" ) @xcite для одновременного изучения @xmath37 и @xmath59. для этой цели мы предлагаем присвоить @xmath59 гамму до @xmath255, а именно @xmath256, потому что полное условное распределение также является гамма и задается @xmath257 \\sim\\ga\\big(\\alpha_{t }, 1/[\\psi(|b_j|/\\sigma ) + \\beta_{t}]\\большой).\\ ] ] напомним, что здесь мы вычисляем полное условное распределение напрямую, используя маргинальный псевдоприоритет @xmath238, потому что используемые нами функции Бернштейна в таблице [ tab : exam ] не могут индуцировать правильные априорные значения. однако, если @xmath238 является правильным, соответствующий нормализующий термин будет опираться на @xmath59. в результате полное условное распределение @xmath59, возможно, больше не является гамма или даже недоступно аналитически.    на рисунке [fig : graphal0]-(a ) изображена иерархическая модель для байесовской линейной регрессии с штрафом, а в таблице [ tab : alg ] приведена процедура ecme, где e - шаг и cm - шаг соответственно идентичны e - шагу и m - шагу алгоритма em, с @xmath258. шаг cme обновляет @xmath59 s с помощью @xmath259, чтобы убедиться, что @xmath260, необходимо предположить, что @xmath261. в следующих экспериментах мы устанавливаем @xmath262. мы проводим эксперименты с предшествующим значением @xmath263 для сравнения. это предшествующее значение вызвано штрафом за норму @xmath264, так что это правильная спецификация. более того, полное условное распределение @xmath59 w.r.t. его гамма, предшествующая @xmath265, по-прежнему является гамма; то есть @xmath257 \\sim\\ga\\big({\\alpha_t}{+}2, \\ ; 1/({\\ beta_t } { + } \\sqrt{|b_j|/\\sigma})\\big).\\ ] ] таким образом, шаг cme для обновления @xmath59 s приведен @xmath266 анализ сходимости алгоритма ecme был представлен @xcite, который доказал, что алгоритм ecme сохраняет свойство монотонности от стандартного em. Более того, также использовался алгоритм ecme, основанный на псевдоприорах автор: @xcite.   . основная процедура алгоритма ecme [ cols= \" \u003c, \u003c \", ] наш анализ основан на наборе смоделированных данных, которые генерируются в соответствии с @xcite. в частности, мы рассматриваем следующие три модели данных: малую, \"среднюю\" и большую. \" данные s : : : @xmath267, @xmath268, @xmath269 и @xmath270 представляют собой матрицу @xmath271 с @xmath272 по диагонали и @xmath273 вне диагонали. данные m : : : @xmath274, @xmath275, @xmath276 содержат @xmath277 ненулевых значений, таких как @xmath278 и @xmath279, а также @xmath280. данные l : : : @xmath281, @xmath282, @xmath283 и @xmath284 (пять блоков ).    для каждой модели данных мы генерируем матрицы данных @xmath285 @xmath286 таким образом, что каждая строка @xmath286 генерируется из многомерного гауссовского распределения со средним значением @xmath287 и ковариационной матрицей @xmath270, @xmath288 или @xmath289. мы предполагаем линейную модель @xmath290 с многомерными гауссовскими предикторами @xmath286 и гауссовскими ошибками. мы выбираем @xmath240 таким образом, чтобы отношение сигнал/шум (snr) было заданным значением. следуя настройке в @xcite, мы используем @xmath291 во всех экспериментах. мы используем стандартизированную ошибку прогнозирования (spe) для оценки способности модели к прогнозированию. минимальное достижимое значение для spe равно @xmath272. точность выбора переменной измеряется правильно предсказанными нулями и неправильно предсказанными нулями в @xmath292. snr и spe определены как @xmath293 для каждой модели данных, мы генерируем обучающие данные размером @xmath294, очень большие данные проверки и тестовые данные, каждый размером @xmath295. для каждого алгоритма оптимальные параметры глобальной настройки выбираются путем перекрестной проверки, основанной на минимизации средних ошибок прогнозирования. используя модель @xmath292, вычисленную на обучающих данных, мы вычисляем spe на тестовых данных. эта процедура повторяется @xmath296 раз, и мы сообщаем среднее и стандартное отклонение spe и среднее значение ошибки от нуля до ненулевого значения. мы используем ` \" для обозначения доли правильно предсказанных нулевых записей в @xmath226, то есть @xmath297; если все ненулевые записи предсказаны правильно, этот показатель должен быть @xmath298. мы приводим результаты в таблице [ tab : toy2 ]. видно, что наша настройка на рисунке [fig : graphal0]-(a ) лучше, чем две другие настройки на рисунках [fig : graphal0]-(b ) и (c) как по точности прогнозирования модели, так и по возможности выбора переменных. особенно, когда размер набора данных принимает большие значения, производительность прогнозирования при второй настройке ухудшается. несколько невыпуклых штрафов являются конкурентоспособными, но они превосходят лассо. более того, мы видим, что log, exp, lfr и cel немного превосходят @xmath264. штраф @xmath264 действительно страдает от проблемы численной нестабильности во время вычислений em. как мы знаем, предварительные значения, индуцированные из lfr, cel и exp, а также log с помощью @xmath299, являются неправильными, но предварительные значения, индуцированные из @xmath264, являются правильными. экспериментальные результаты показывают, что эти неподходящие предварительные данные работают хорошо, даже лучше, чем в правильном случае.     против @xmath300 для данных s \" и данных m \", где @xmath301 - это перестановка @xmath302 таким образом, что @xmath303. ] напомним, что в нашем подходе каждая регрессионная переменная @xmath37 соответствует отдельному локальному параметру настройки @xmath59. таким образом, интересно эмпирически исследовать внутреннюю взаимосвязь между @xmath37 и @xmath59. пусть @xmath304 - оценка @xmath59, полученная с помощью нашего алгоритма ecme (alg 1 \" ), а @xmath305 - перестановка @xmath306 таким образом, что @xmath307. на рисунке [ fig : tb1 ] показано изменение @xmath308 vs.@xmath300 с помощью log, exp, lfr и cel для данных s \" и данных m\". мы видим, что @xmath308 уменьшается по времени. Более того, @xmath308 становится равным 0, когда @xmath300 принимает некоторое большое значение. аналогичное явление также наблюдается для данных l. \"таким образом, это показывает, что субординатор является мощным байесовским подходом к выбору переменных. в этой статье мы ввели субординаторы в определение невыпуклых штрафных функций. это приводит нас к байесовскому подходу для построения псевдоприоров, вызывающих разреженность. в частности, мы проиллюстрировали использование двух составных пуассоновских субординаторов: составного гамма-субординатора Пуассона и отрицательного биномиального субординатора. кроме того, мы установили взаимосвязь между двумя семействами составных пуассоновских субординаторов. то есть мы доказали, что два семейства составных пуассоновских субординаторов имеют одинаковое ограничивающее поведение. более того, их плотности в каждый момент времени имеют одинаковое среднее значение и дисперсию. мы разработали алгоритмы ecme для решения задач разреженного обучения, основанные на невыпуклых логарифмах, exp, lfr и штрафах cel. мы провели экспериментальное сравнение с современным подходом. результаты показали, что наш подход к невыпуклому наказанию потенциально полезен в байесовском моделировании высокой размерности. наш подход может быть применен к системе точечной оценки. также интересно использовать полностью байесовскую систему, основанную на оценке mcmc. мы хотели бы рассмотреть этот вопрос в будущей работе. учтите, что @xmath310 & = \\log\\big[1-\\frac{1}{1{+}\\rho } \\exp(-\\frac{\\rho}{1{+}\\rho } \\gamma s)\\big] - \\log\\big[1-\\frac{1}{1{+}\\rho}\\big ] \\\\ & = \\sum_{k=1}^{\\infty } \\frac{1}{k ( 1{+}\\rho)^k } \\big[1- \\exp\\big ( { -}\\frac{\\rho}{1{+}\\rho } k \\gamma s\\big)\\big ] \\\\ & = \\sum_{k=1}^{\\infty } \\frac{1}{k ( 1{+}\\rho)^k } \\int_{0}^{\\infty } ( 1- \\exp(- u s ) ) \\delta_{\\frac{\\rho k \\gamma}{1{+}\\rho}}(u ) d u.\\end{выровнено}\\ ] ] таким образом, мы имеем, что @xmath311. здесь мы приводим альтернативное доказательство утверждения [pro:8]-(d), которое непосредственно вытекает из следующей леммы. пусть @xmath312 принимает дискретное значение в @xmath313 и следует отрицательному биномиальному распределению @xmath314. если @xmath315 сходится к положительной константе как @xmath316, @xmath317 сходится в распределении к гамма-случайной величине с формой @xmath315 и масштабом @xmath272. поскольку @xmath318 у нас есть, что @xmath319 обратите внимание, что @xmath320 и @xmath321 это приводит нас к @xmath322 аналогично, у нас есть, что @xmath323 рассматривает смесь @xmath324 со смешиванием @xmath325. то есть, @xmath326 допускает @xmath327, @xmath328 и @xmath329, у нас есть @xmath330, теперь мы рассматриваем смесь @xmath331 с @xmath332, которая является @xmath333. пусть @xmath334, @xmath335, @xmath336 и @xmath337. таким образом, @xmath338, поскольку @xmath339=1 $ ], нам нужно только рассмотреть случай, когда @xmath213. напомним, что @xmath119, среднее значение и дисперсия которого равны @xmath340 всякий раз, когда @xmath213. по неравенству Чебышева, мы имеем, что @xmath341 следовательно, мы имеем, что @xmath342 аналогично, у нас есть часть (b). сначала мы отмечаем, что @xmath343, что подразумевает, что @xmath344 для @xmath345. впоследствии мы имеем, что @xmath346 \\leq0 $ ]. в результате, @xmath347 для @xmath345. что касается @xmath348, то оно непосредственно получено из этого @xmath349, поскольку @xmath350 = \\frac{\\gamma}{\\exp(\\gamma s ) } - \\frac{\\gamma}{1+\\gamma s}\u003c0 $ ] для @xmath345, мы имеем, что @xmath351 для @xmath345. сначала рассмотрим, что @xmath352 \\prod_{j=1}^p \\sigma ^{-1 } \\exp\\big(-t_j \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\big).\\ ] ] чтобы доказать, что @xmath353 является правильным, достаточно получить, что @xmath354 \\prod_{j=1}^p \\sigma ^{-1 } \\exp \\big(-t_j \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\big ) d { \\bf b } \u003c \\infty}.\\ ] ] непосредственно вычисляется, что @xmath355 \\nonumber \\\\ & = \\exp\\big [ { - } \\frac{1}{2 \\sigma } ( { \\bf b}{- } { \\bf z})^t { \\bf x}^t { \\bf x } ( { \\bf b}- { \\bf z } ) \\big ] \\times\\exp\\big[- \\frac{1}{2 \\sigma } { \\bf y}^t ( { \\bf i}_n - { \\bf x } ( { \\bf x}^t { \\bf x})^{+ } { \\bf x}^t ) { \\bf y}\\big],\\end{выровнено}\\ ] ] где @xmath356 и @xmath357 - это псевдоинверсия матрицы Мура - Пенроуза @xmath358 @xcite. здесь мы используем хорошо зарекомендовавшие себя свойства @xmath359 и @xmath360. обратите внимание, что если @xmath358 не является сингулярным, то @xmath361. в этом случае мы рассматриваем обычное многомерное нормальное распределение @xmath362. в противном случае мы рассматриваем сингулярное многомерное нормальное распределение @xmath363 @xcite, плотность которого задается @xmath364.\\ ] ] здесь @xmath365 и @xmath366, @xmath367 являются положительными собственными значениями @xmath358. в любом случае, мы всегда пишем @xmath368. таким образом, @xmath369 d{\\bf b } \u003c \\infty}$ ]. тогда это соответствует требованиям @xmath370, потому что @xmath371 \\prod _ { j=1}^p \\exp\\big ( { - } t_j \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\big)\\leq\\exp\\big [ { - } \\frac{1}{2 \\sigma } \\|{\\bf y}- { \\bf x}{\\bf b}\\|_2 ^ 2 \\big].\\ ] ] теперь мы считаем, что @xmath372 \\prod_{j=1}^p \\exp\\big(-t_j \\psi \\big(\\frac{|b_j| } { \\sigma } \\big ) \\big).\\ ] ] пусть @xmath373 { \\bf y}$ ]. поскольку матрица @xmath374 положительно полуопределена, мы получаем @xmath375. основываясь на выражении ( [ eqn : pf01 ] ), мы можем записать @xmath376 \\varpropto n({\\bf b}|{\\bf z }, \\сигма({\\bf x}^t { \\bf x})^{+ } ) { \\iga}(\\sigma|\\frac{\\alpha_{\\sigma } { + } n{+}2p{-}q}{2 }, \\nu{+ } \\beta_{\\sigma}).\\ ] ] следовательно, мы имеем, что @xmath377 d { \\bf b } d \\sigma } \u003c \\infty,\\ ] ] и, следовательно, @xmath377 \\prod_{j=1}^p \\exp\\big(-t_j \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\big ) d { \\bf b}d \\sigma } \u003c \\infty.\\ ] ] следовательно, @xmath378 является правильным. в-третьих, мы берем @xmath379 } { \\sigma ^{\\frac{n+\\alpha_{\\sigma}+2p}{2 } + 1 } } \\prod_{j=1}^p \\big\\{\\exp \\big({-}t_j \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\большой ) \\frac { t_j^{{\\alpha_t}{- } 1 } \\exp({- } { \\beta_t } t_j)}{\\gamma({\\alpha_t } ) } \\big\\ } \\\\ & \\triangleq f({\\bf b }, \\sigma, { \\bf t}).\\end{выровнено}\\ ] ] в этом случае мы вычисляем @xmath380 } { \\sigma^{\\frac{n+\\alpha_{\\sigma}+2p}{2 } + 1 } } \\prod _ { j=1}^p \\frac{1 } { \\big({\\beta_t } { + } \\psi\\big(\\frac{|b_j| } { \\sigma } \\big ) \\big)^{{\\alpha_t } } } d { \\bf b}d \\sigma}.\\ ] ] аналогично предыдущему доказательству, мы также есть этот @xmath381, потому что @xmath382. в результате, @xmath383 является правильным. наконец, рассмотрим настройку @xmath384. то есть @xmath385 и @xmath386. в этом случае, если @xmath387, мы получаем @xmath388 и @xmath389. в результате мы используем обратное гамма-распределение @xmath390. таким образом, результаты остаются в силе. 

в этой статье мы обсуждаем байесовское невыпуклое наказание для задач разреженного обучения. мы исследуем непараметрическую формулировку для параметров скрытой усадки с использованием субординаторов, которые являются одномерными процессами lvy. в частности, мы изучаем семейство непрерывных составных субординаторов Пуассона и семейство дискретных составных субординаторов Пуассона. мы приводим в качестве примера четыре конкретных субординатора: гамма, пуассоновский, отрицательный биномиальный и квадратный субординаторы Бесселя. показатели Лапласа субординаторов являются функциями Бернштейна, поэтому их можно использовать в качестве невыпуклых штрафных функций, вызывающих разреженность. мы используем эти субординаторы в задачах регрессии, получая иерархическую модель с множеством параметров регуляризации. мы разрабатываем алгоритмы ecme (математическое ожидание / условная максимизация) для одновременной оценки коэффициентов регрессии и параметров регуляризации. эмпирическая оценка смоделированных данных показывает, что наш подход осуществим и эффективен при анализе многомерных данных.