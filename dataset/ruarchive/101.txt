выбор модели является важной проблемой во многих областях, включая машинное обучение. если не выбрана подходящая модель, любые усилия по оценке параметров или прогнозированию результата алгоритма безнадежны. учитывая набор моделей-кандидатов, цель выбора модели состоит в том, чтобы выбрать модель, которая наилучшим образом аппроксимирует наблюдаемые данные и отражает лежащие в их основе закономерности. критерии выбора модели определены таким образом, чтобы они обеспечивали баланс между _ качеством соответствия (gof) _ и _ обобщаемостью _ или _ сложностью _ моделей. качество соответствия измеряет, насколько хорошо модель отражает закономерность в данных. обобщаемость/ сложность - это оценка эффективности модели на основе невидимых данных или того, насколько точно модель соответствует /предсказывает будущие данные. модели с более высокой сложностью, чем необходимо, могут пострадать от переобучения и плохого обобщения, в то время как слишком простые модели будут недостаточно приспособлены и иметь низкую эффективность. перекрестная проверка @xcite, самонастройка @xcite, информационный критерий акайке (aic) @xcite и байесовский информационный критерий (bic) @xcite являются хорошо известными примерами традиционного выбора модели. при методах повторной выборки, таких как перекрестная проверка и самонастройка, ошибка обобщения модели оценивается с помощью моделирования методом монте-карло. в отличие от методов повторной выборки, методы выбора модели, такие как aic и bic, не требуют проверки для вычисления ошибки модели и являются вычислительно эффективными. в этих процедурах \"информационный критерий\" определяется таким образом, что ошибка обобщения оценивается путем наложения штрафа за ошибку модели на наблюдаемые данные. было введено большое количество информационных критериев с различными мотивациями, которые приводят к различным теоретическим свойствам. например, более жесткий параметр штрафования в bic благоприятствует более простым моделям, в то время как aic работает лучше, когда набор данных имеет очень большой размер выборки. методы ядра - это мощные, вычислительно эффективные аналитические инструменты, способные работать с данными большой размерности со сколь угодно сложной структурой. они успешно применяются в широком спектре приложений, таких как классификация и регрессия. в методах ядра данные отображаются из их исходного пространства в пространство признаков более высокой размерности, воспроизводящее гильбертово пространство ядра (rkhs). идея, стоящая за этим отображением, заключается в преобразовании нелинейных взаимосвязей между точками данных в исходном пространстве в простую для вычисления линейную задачу обучения в пространстве признаков. например, в регрессии ядра переменная отклика описывается как линейная комбинация встроенных данных. любой алгоритм, который может быть представлен с помощью точечных продуктов, имеет оценку ядра. эта операция, называемая кернелизацией, позволяет преобразовать традиционные, уже проверенные методы выбора модели в более сильные, соответствующие методы ядра. литература по методам ядра, однако, в основном сосредоточена на выборе ядра и настройке параметров ядра, но проводится лишь ограниченная работа по выбору модели на основе ядра @xcite. в этом исследовании мы исследуем информационный критерий на основе ядра для моделей регрессии гребня. в kernel ridge regression (krr) целью критерия выбора модели ядра является настройка параметров хребта для нахождения наиболее прогностического подпространства по отношению к имеющимся данным и невидимым данным.    в классических методах выбора модели эффективность критерия выбора модели оценивается теоретически путем предоставления доказательства согласованности, когда размер выборки стремится к бесконечности, и эмпирически с помощью имитационных исследований для конечных размеров выборки. другие методы исследуют вероятностную верхнюю границу ошибки обобщения @xcite. доказательство свойств согласованности выбора модели в _ kernel model selection _ является сложной задачей. процедура доказательства классическими методами здесь не работает. вот некоторые причины этого: размер модели для оценки таких проблем, как недостаточная / избыточная подгонка @xcite, не очевиден (для точек данных @xmath1 размерности @xmath2 ядром является @xmath3, который не зависит от @xmath2), и асимптотические вероятности ошибки обобщения или оценок трудно вычислить в rkhs. исследователи обобщили традиционные критерии выбора модели и эмпирически продемонстрировали успешность их выбора модели ядра. кобаяши и Комаки @xcite извлекли основанный на ядре информационный критерий регуляризации (kric), используя уравнение на собственные значения для задания параметров регуляризации в ядре логистической регрессии и машин опорных векторов (svm). rosipal и др. @xcite разработал информационный критерий ковариации (cic) для выбора модели в анализе главных компонент ядра из-за его превосходящих результатов по сравнению с aic и bic в ортогональной линейной регрессии. демьянов и др. @xcite, предоставил альтернативный способ вычисления функции правдоподобия в информационном критерии акайке (aic, @xcite) и байесовском информационном критерии (bic, @xcite) и использовал его для выбора параметров в svms с использованием гауссовского ядра.    как указал ван Эмден @xcite, желательной моделью является модель с наименьшим количеством зависимых переменных. таким образом, определение термина сложности, который измеряет взаимозависимость параметров модели, позволяет выбрать наиболее желательную модель. в этом исследовании мы определяем новую дисперсию по переменным и получаем меру сложности как аддитивную комбинацию ядер, определенных для параметров модели. формализация термина сложности таким образом эффективно отражает взаимозависимость каждого параметра модели. мы называем этот новый метод информационным критерием на основе ядра (kic).    критерий выбора модели в регрессии гауссовского процесса (gpr ; @xcite) и информационная сложность на основе ядра (icomp ; @xcite) напоминают kic в использовании меры сложности, основанной на ковариации. однако методы различаются, поскольку эти показатели сложности отражают взаимозависимость между точками данных, а не параметрами модели. хотя мы не можем теоретически установить свойства согласованности kic, мы эмпирически оцениваем эффективность kic как на синтетических, так и на реальных наборах данных, получая самые современные результаты по сравнению с перекрестной проверкой без исключения (loocv), icomp на основе ядра и максимальной логарифмической вероятностью в георадар. статья организована следующим образом. в разделе [ sec : krr ] мы даем обзор регрессии гребня ядра. kic подробно описан в разделе [ sec : kic ]. раздел [ sec : om ] содержит краткое объяснение методов, с которыми сравнивается kic, а в разделе [ sec : exp ] мы оцениваем эффективность kic с помощью наборов экспериментов. в регрессионном анализе регрессионная модель имеет вид : @xmath4, где @xmath5 может быть как линейной, так и нелинейной функцией.    в линейной регрессии мы имеем @xmath6, где @xmath7 - вектор наблюдения (переменная ответа) размера @xmath8, @xmath9 - матрица данных полного ранга независимых переменных размера @xmath10, а @xmath11 - неизвестный вектор параметров регрессии, где @xmath12 обозначает перестановку. мы также предполагаем, что вектор ошибки (шума) @xmath13 является @xmath1-мерным вектором, элементы которого рисуются i.i.d, @xmath14, где @xmath15 - это @xmath1-мерная единичная матрица, а @xmath16 - неизвестная дисперсия. коэффициенты регрессии минимизируют квадраты ошибок, @xmath17, между оценочной функцией @xmath18 и целевой функцией @xmath5. когда @xmath19, проблема некорректно поставлена, так что требуется какая-то регуляризация, такая как регуляризация Тиханова (регрессия гребня), и коэффициенты минимизируют следующую оптимизационную задачу @xmath20, где @xmath21 - параметр регуляризации. оценочные коэффициенты регрессии в ridge regression @xmath22 следующие: @xmath23 в _ kernel _ ridge regression ( krr ) матрица данных @xmath9 нелинейно преобразуется в rkhs с использованием карты объектов @xmath24. оценочные коэффициенты регрессии, основанные на @xmath25, следующие: @xmath26, где @xmath27 - матрица ядра. уравнение [ eq : theta ] не получает явного выражения для @xmath28 из-за @xmath24 (трюк ядра позволяет избежать явного определения @xmath25, которое может быть численно неразрешимым при вычислении в rkhs, если оно известно ), таким образом, используется оценка гребня (например, @xcite ), которая исключает @xmath24 : @xmath29 использование @xmath30 при вычислении krr аналогично регуляризации функции регрессии вместо коэффициентов регрессии, где целевая функция равна : @xmath31 и @xmath32 обозначают соответствующие rkhs. для @xmath33 и @xmath34 у нас есть : @xmath35, где @xmath36 - функция ядра, и @xmath37. основным вкладом этого исследования является введение нового информационного критерия на основе ядра (kic) для выбора модели в регрессии на основе ядра. согласно уравнению, kic обеспечивает баланс между степенью соответствия и сложностью модели. gof определяется с использованием функции, основанной на логарифмическом правдоподобии (мы максимизируем штрафное логарифмическое правдоподобие), а мера сложности - это функция, основанная на функции ковариации параметров модели. в следующих подразделах мы подробно рассмотрим эти термины. определение ван Эмдена @ xcite для меры сложности случайного вектора основано на взаимодействиях между случайными величинами в соответствующей ковариационной матрице. желательной моделью является модель с наименьшим количеством зависимых переменных. это уменьшает информационную энтропию и приводит к снижению сложности. в этой статье мы сосредоточимся на этом определении показателей сложности.    учитывая нормальное распределение @xmath2-переменной @ xmath38, сложность ковариационной матрицы @ xmath39 определяется энтропией Шеннона @xcite, @xmath40, где @xmath41, @xmath42 - предельная и совместная энтропия, а @xmath43 - диагональный элемент @xmath44 из @xmath39. @xmath45 тогда и только тогда, когда ковариаты независимы. мера сложности в уравненииn изменяется при ортонормированных преобразованиях, поскольку оно зависит от координат векторов случайных величин @xmath46 @xcite. чтобы преодолеть эти недостатки, Бозодган и Хотон @xcite ввели информационный критерий icomp с мерой сложности, основанной на максимальной сложности ковариации, которая является верхней границей меры сложности в уравнении: @xmath47 эта мера сложности пропорциональна оцененному арифметическому (@xmath48 ) и среднему геометрическому (@xmath49) значению собственные значения ковариационной матрицы. большие значения @xmath50 указывают на более высокую зависимость между случайными величинами, и наоборот. чжан @xcite представил форму ядра этой меры сложности @xmath50, которая вычисляется на основе ковариации оценки гребня на основе ядра: @xmath51 мера сложности в регрессии гауссовского процесса (gpr ; @xcite) определяется как @xmath52, концепция из совместной энтропии @xmath42 (как показано в уравнение [ eq : сложность ] ). в отличие от icomp и георадара, мера сложности в kic определяется с использованием нормы Гильберта - Шмидта (hs) ковариационной матрицы, @xmath53. минимизируя этот показатель сложности, мы получаем модель с большим количеством независимых переменных.    в следующих разделах мы подробно объясняем, как определить необходимую дисперсию в зависимости от переменной в показателе сложности, а также вычисление показателя сложности. + в методах выбора модели на основе ядра, таких как icomp и gpr, мера сложности определяется на основе ковариационной матрицы, которая имеет размер @xmath54 для @xmath9 размера @xmath10. идея, стоящая за этой мерой, заключается в вычислении взаимозависимости между параметрами модели, которая не зависит от количества параметров модели @xmath2. другими словами, концепция размера модели скрыта из-за определения ядра. чтобы получить показатель сложности, зависящий от @xmath2, мы вводим дисперсию по переменным, используя аддитивную комбинацию ядер для каждого параметра модели. пусть @xmath55 - вектор параметров регрессии гребня ядра : @xmath56, где @xmath57 и @xmath58, и @xmath59 решение krr задается @xmath60. величина @xmath61 = \\sigma^2 \\operatorname{tr}[k(k+\\alpha i)^{-2 } ] $ ] может быть интерпретирована как сумма отклонений для векторов параметров по компонентам, если введена следующая сумма по компонентам ядра : @xmath62 где @xmath63 и @xmath64 обозначают j -ю компоненту векторов @xmath65 и @xmath66. с этим ядром sum функция @xmath67 может быть записана как : @xmath68, где @xmath69 - это функция в @xmath70, rkhs, определенная @xmath71. параметр @xmath28 в этом случае задается @xmath72, где @xmath73, и, таким образом, @xmath69 в уравнении [ eq : g ] равно @xmath74. пусть @xmath75 - условная ковариация @xmath74 или @xmath69, заданная @xmath76. у нас есть @xmath77,\\end{выровнено}\\ ] ] где @xmath78 - матрица грама с @xmath71. начиная с @xmath79, у нас есть @xmath80 = \\operatorname{tr}[\\sigma_{\\theta}].\\end{выровнено}\\ ] ] формализация термина сложности с переменной дисперсией эффективно отражает взаимозависимость каждого параметра модели (измеряет значимость вклада переменных ) явно. + греттон и др. @xcite представил меру независимости, основанную на ядре, а именно критерий независимости Гильберта - Шмидта (hsic), который объясняется здесь. предположим, что @xmath81 и @xmath82 являются случайными векторами с отображениями объектов @xmath83 и @xmath84, где @xmath85 и @xmath86 - это rkhss. оператор перекрестной ковариации, соответствующий совместному распределению вероятностей @xmath87, является линейным оператором, @xmath88 таким, что : @xmath89,\\end{выровнено}\\ ] ] где @xmath90 обозначает тензорное произведение, @xmath91= e[k(\\cdot, x)]$ ], и @xmath92=e[k(\\cdot, y)]$ ], для @xmath93 и связанной с ним функции ядра @xmath36. мера hsic для разделимых rkhs @xmath85 и @xmath86 является квадратичной hs - нормой оператора перекрестной ковариации и обозначается как : @xmath94\\end{выровнено}\\ ] ] * теорема 1. * предположим, что @xmath95 и @xmath96 компактны для всех @xmath97 и @xmath98, @xmath99 и @xmath100, @xmath101 тогда и только тогда, когда @xmath102 и @xmath7 независимы (теорема 4 в @xcite). + вычисляя hsic по ковариационной матрице, связанной с параметрами модели s @xmath103, мы можем измерить независимость между параметрами. поскольку @xmath104 является симметричной положительной полуопределенной матрицей, @xmath105, а след нормы hs ковариационной матрицы равен : @xmath106 = \\sum_{j=1}^p v_j^2\\nonumber\\\\ ~~&= \\sigma^4 \\operatorname{tr}[k(k+\\alpha i)^{-2 } k(k+\\alpha i)^{-2}]\\end{выровнено}\\ ] ] kic определяется как : @xmath107, где @xmath108 - термин сложности, основанный на уравнении [ eq : hs ]. нормализация с помощью @xmath109 позволяет получить показатель сложности, устойчивый к изменениям дисперсии (аналогично критерию icomp). минимальный коэффициент полезного действия определяет наилучшую модель. ]. штрафное логарифмическое правдоподобие (pll) в krr для данных с нормальным распределением определяется следующим образом: @xmath110 неизвестные параметры @xmath22 и @xmath111 вычисляются путем минимизации целевой функции kic. @xmath112 мы также исследовали эффект использования @xmath113 $ ] и @xmath61 $ ] в качестве терминов сложности. эмпирические результаты, представленные в подразделе [subsec : realdata ] на реальных наборах данных, и сравненные с kic. мы обозначаем эти информационные критерии как: @xmath114,\\end{выровнено}\\ ] ] @xmath115.\\end{выровнено}\\ ] ] как в kic_1, так и в kic_2, аналогично kic, @xmath116, в то время как, поскольку термин сложности зависит от @xmath16, @xmath111 для kic_1 является: @xmath117=0.\\end{выровнено}\\ ] ] если мы обозначим @xmath118, @xmath111 - это решение задачи квадратичной оптимизации, @xmath119, где @xmath120. в случае kic_2 @xmath111 является реальным корнем следующей кубической задачи: @xmath121, где @xmath122 $ ]. мы сравнили kic с loocv @xcite, icomp на основе ядра @xcite и maximum log of marginal likelihood в gpr (сокращенно gpr) @xcite, чтобы найти оптимальные регрессоры гребня. причина сравнения kic с icomp и gpr заключается в том, что во всех этих методах мера сложности вычисляет взаимозависимость параметров модели как функцию ковариационной матрицы различными способами. loocv является стандартным и часто используемым методом выбора модели. * loocv: * повторная выборка методов выбора модели, таких как перекрестная проверка, отнимает много времени @xcite. например, перекрестная проверка без исключения (loocv) имеет вычислительную стоимость, равную @xmath123 количеству комбинаций параметров ( @xmath124 - это время обработки алгоритма выбора модели @xmath125) для обучающих выборок @xmath126. чтобы иметь методы перекрестной проверки с более быстрым временем обработки, предусмотрена формула закрытой формы для оценки риска алгоритма при особых условиях. мы рассматриваем основанную на ядре замкнутую форму loocv для линейной регрессии, представленную @xcite: @xmath127^{-1}[i - h]y\\|_2 ^ 2}{n}\\end{выровнено}\\ ] ] где @xmath128 - матрица hat.    * максимизация логарифма предельного правдоподобия (gpr ) * - это метод регрессии, основанный на ядре. для данного обучающего набора @xmath129 и @xmath130 многомерное гауссово распределение определено для любой функции @xmath5 таким образом, что @xmath131, где @xmath39 - ядро. предельное правдоподобие используется в качестве критерия выбора модели в георадаре, поскольку оно балансирует между отсутствием соответствия и сложностью максимизируя логарифм предельного правдоподобия, получаем оптимальные параметры для выбора модели. логарифм предельного правдоподобия обозначается как : @xmath132, где @xmath133 обозначает соответствие модели, @xmath134 обозначает сложность, а @xmath135 - константа нормализации. без потери общности в данной статье георадар означает, что в георадаре используется критерий выбора модели. * icomp: * icomp на основе ядра, представленный в @xcite, является информационным критерием для выбора моделей и определяется как @xmath136, где @xmath50 и @xmath39, разработанные в уравнениях [ eq : cicomp ] и [ eq : sigmaicomp ]. в этом разделе мы оцениваем производительность kic на синтетических и реальных наборах данных и сравнить с конкурирующими методами выбора модели. kic был впервые оценен в задаче аппроксимации @xmath137 из набора из 100 точек, отобранных через регулярные промежутки времени в @xmath138 $]. чтобы оценить устойчивость к шуму, к функции @xmath139 был добавлен обычный случайный шум при двух соотношениях шум/сигнал (nsr): @xmath140 и @xmath141. на рисунке [ sinc ] показана функция sinc и возмущенные наборы данных. были проведены следующие эксперименты: (1 ) показано, как kic балансирует между gof и сложностью, ( 2 ) показано, как kic и mse на обучающих наборах изменяются при изменении размера выборки и уровня шума в данных, (3 ) исследуется эффект использования разных ядер и (4 ) оценивается эффективность согласованность kic при выборе параметров. все эксперименты были проведены 100 раз с использованием случайно сгенерированных наборов данных и соответствующих тестовых наборов размером 1000.    * эксперимент 1. * влияние @xmath21 на сложность, несоответствие и значения kic было измерено с помощью настройки @xmath142, при этом модели krr генерировались с использованием гауссовского ядра с различными стандартными отклонениями, @xmath143, вычисленного по 100 точкам данных. результаты показаны на рисунке [ co_la_kic ]. модель, сгенерированная с помощью @xmath144, не подходит, поскольку она чрезмерно сложна, в то время как @xmath145 дает более простую модель, которая не подходит. по мере увеличения параметра ridge @xmath21 сложность модели уменьшается, в то время как качество соответствия отрицательно сказывается. kic балансирует между этими двумя условиями, что дает критерий для выбора модели, которая обладает хорошим обобщением, а также хорошим соответствием данным. * эксперимент 2. * влияние размера обучающей выборки было исследовано путем сравнения размеров выборки, @xmath1, 50 и 100, в общей сложности для четырех наборов экспериментов: ( @xmath146 ) : ( @xmath147 ), ( @xmath148 ), ( @xmath149 ), ( @xmath150 ). гауссово ядро использовалось с @xmath151. значение kic и mошибка в квадрате ean ( mse, @xmath152 ) для разных @xmath153 @xmath154 показана на рисунке [ kic - mse ]. данные с nsr=@xmath141 имеют большие значения mse и большие столбцы ошибок и, следовательно, большие значения kic по сравнению с данными с nsr=@xmath140. в обоих случаях kic и mse изменяются с одинаковыми профилями относительно @xmath21. шум и размер выборки не влияют на kic для выбора наилучшей модели (параметр @xmath21). * эксперимент 3. * был исследован эффект использования гауссовского ядра @xmath155 по сравнению с ядром Коши @xmath156, где @xmath157 и @xmath158 при вычислении критериев выбора модели на основе ядра icomp, kic, gpr и loocv. результаты представлены на рисунках [ ядро Гаусса ] и [ядро Коши ]. на графиках показаны прямоугольные графики с маркерами в @xmath159 и @xmath160 эмпирических распределений значений mse. как и ожидалось, mse всех методов больше, когда nsr высокий, @xmath161, и меньше для большего из двух обучающих наборов (100 выборок). loocv, icomp и kic показали сопоставимые результаты и даже лучше, чем георадар, использующий гауссово ядро для обработки данных с nsr @xmath162. в других случаях наилучшие результаты (наименьший mse) были достигнуты с помощью kic. все методы имеют меньшие значения mse при использовании ядра Гаусса по сравнению с ядром Коши. георадар с ядром Коши дает результаты, сопоставимые с kic, но со стандартным отклонением, близким к нулю. * эксперимент 4. * мы оценили согласованность выбора/настройки параметров моделей по сравнению с loocv. мы рассмотрели четыре эксперимента с размером выборки, @xmath163 и nsr @xmath164. параметры для настройки или выбора - это @xmath165, @xmath166 и @xmath167 для гауссовского ядра. частота выбора параметров показана на рисунке [ loocv ] для loocv и на рисунке [ kic_frequency ] для kic. более концентрированная частота показывает более согласованный критерий выбора. диаграммы показывают, что kic более последователен в выборе параметров, чем loocv. loocv также чувствителен к размеру выборки. он обеспечивает более согласованный результат для тестов с выборками @xmath168. + мы использовали три контрольных показателя, выбранных из наборов данных delve ( www.cs.toronto.edu /~delve/data ) : ( 1 ) набор данных abalone ( 4177 экземпляров, 7 измерений ), ( 2 ) семейство наборов данных kin (4 набора данных ; 8192 экземпляра, 8 измерений) и ( 3 ) семейство наборов данных puma ( 4 набора данных ; 8192 экземпляра, 8 измерений ).    для набора данных abalone задача состоит в оценке возраста морских ушек. мы использовали нормализованные атрибуты в диапазоне [ 0,1 ]. эксперимент повторяют 100 раз, чтобы получить доверительный интервал. в каждом испытании 100 образцов были выбраны случайным образом в качестве обучающего набора, а остальные 4077 образцов - в качестве тестового набора. наборы данных kin - family и puma - family представляют собой реалистичное моделирование руки робота, учитывающее комбинации таких атрибутов, как нелинейность движения руки (n) или достаточно линейность (f), а также уровень шума (непредсказуемости) в данных: средний (m) или высокий ( h ). семейство kin включает в себя: наборы данных kin-8fm, kin-8fh, kin-8 nm, kin-8nh, а семейство puma содержит: наборы данных puma-8fm, puma-8fh, puma-8 nm и puma-8nh.    в наборах данных семейства kin, содержащих угловые положения 8-звеньевого манипулятора робота, прогнозируется расстояние конечного исполнительного элемента манипулятора робота от исходного положения. угловое положение звена манипулятора робота прогнозируется с учетом угловых положений, угловых скоростей и крутящих моментов звеньев. мы сравнили kic_1 ( [ eq : kic1 ] ), kic_2 ( [ eq : kic2 ] ) и kic с loocv, icomp и gpr на трех наборах данных. результаты показаны в виде прямоугольных графиков на рисунках [ abalone ], [ kin - family ] и [ puma - family ] для наборов данных abalone, kin - family и puma - family, соответственно. наилучшие результаты по всем трем наборам данных были достигнуты с использованием kic, а вторые лучшие результаты были получены для loocv.    для набора данных abalone были получены сопоставимые результаты для kic и loocv, которые лучше, чем icomp, и наименьшее значение mse, полученное с помощью sgpr. kic_1 и kic_2 имели аналогичные значения mse, которые больше, чем для других методов.    для наборов данных семейства kin, за исключением kin-8fm, kic дает лучшие результаты, чем gpr, icomp и loocv. kic_1 и kic_2 дают лучшие результаты, чем gpr и loocv для kin-8fm и kin-8 nm, которые являются наборами данных со средним уровнем шума, но большим mse значение для наборов данных с высоким уровнем шума (kin-8fh и kin-8nh ).    для наборов данных семейства puma kic получил наилучшие результаты по всем наборам данных, за исключением puma-8 nm, где наименьший mse был достигнут с помощью loocv. результат kic сопоставим с icomp и лучше, чем у георадара для набора данных puma-8 нм. для puma-8fm, puma-8fh и puma-8nh, хотя медиана mse для loocv и георадара сопоставима с kic, kic имеет более значительный mse (меньший межквартильный интервал в таблице bots). среднее значение mse для kic_1 и kic_2 ближе к средним значениям mse других методов на puma-8fm и puma-8 nm, где уровень шума умеренный по сравнению с puma-8fh и puma-8nh, где уровень шума высокий. чувствительность kic_1 и kic_2 к шуму обусловлена существованием различий в их формуле. kic_2 имеет больший интерквартильный показатель mse, чем kic_1 в наборах данных с высоким уровнем шума, что подчеркивает эффект @xmath109 в его формуле ( уравнение [ eq : kic2 ] ), а не @xmath16 в уравнении . мы ввели новый информационный критерий на основе ядра (kic) для выбора модели в регрессионном анализе. мера сложности в kic определяется на основе дисперсии по переменным, которая явно вычисляет взаимозависимость каждого параметра, задействованного в модели; тогда как в таких методах, как icomp на основе ядра и gpr, эта взаимозависимость определяется на основе ковариационной матрицы, которая скрывает истинный вклад параметров модели. мы предоставили эмпирические данные, показывающие, как kic превосходит loocv (с формулой оценки в закрытой форме на основе ядра), icomp на основе ядра и георадар, как на искусственных данных, так и на реальных наборах эталонных данных: abalon, kin family и puma family. в этих экспериментах kic эффективно уравновешивает качество подгонки и сложность модели, устойчив к шуму (хотя для более высокого уровня шума у нас, как и ожидалось, больший доверительный интервал) и размеру выборки, последователен в настройке /выборе параметров гребня и ядра и имеет значительно меньшие или сопоставимые среднеквадратичные значения с уважение к конкурирующим методам, в то же время приводящее к более сильным регрессорам. также был исследован эффект от использования разных ядер, поскольку определение правильного ядра играет важную роль в методах ядра. kic продемонстрировал превосходную производительность при использовании разных ядер, а для правильного был получен меньший mse. 

в этой статье вводится основанный на ядре информационный критерий (kic) для выбора модели в регрессионном анализе. новый основанный на ядре показатель сложности в kic эффективно вычисляет взаимозависимость между параметрами модели с использованием дисперсии по переменным и позволяет выбирать лучшие, более надежные регрессоры. экспериментальные результаты показывают превосходную производительность как на моделируемых, так и на реальных наборах данных по сравнению с перекрестной проверкой без исключения (loocv), информационной сложностью на основе ядра (icomp) и максимальным логарифмом предельного правдоподобия в регрессии гауссовского процесса (gpr).