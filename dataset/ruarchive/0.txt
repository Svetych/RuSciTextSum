аддитивные модели @xcite представляют собой важное семейство моделей для полупараметрической регрессии или классификации. некоторыми причинами успеха аддитивных моделей являются их повышенная гибкость по сравнению с линейными или обобщенными линейными моделями и их повышенная интерпретируемость по сравнению с полностью непараметрическими моделями. хорошо известно, что хорошие оценки в аддитивных моделях, как правило, менее подвержены проклятию высокой размерности, чем хорошие оценки в полностью непараметрических моделях. многие примеры таких оценок относятся к большому классу регуляризованных методов, основанных на ядре, над воспроизводящим гильбертовым пространством ядра @xmath0, см., например, @xcite. в последние годы было опубликовано много интересных результатов о скорости обучения регуляризованных моделей на основе ядра для аддитивных моделей, когда основное внимание уделяется разреженности и когда используется классическая функция потерь наименьших квадратов, см., например, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite и ссылки в нем. конечно, функция потерь наименьших квадратов дифференцируема и обладает многими приятными математическими свойствами, но она только локально непрерывна по Липшицу, и поэтому методы, основанные на регуляризованном ядре, основанные на этой функции потерь, обычно страдают плохими свойствами статистической надежности, даже если ядро ограничено. это резко контрастирует с методами ядра, основанными на функции непрерывных потерь Липшица и на ограниченной функции потерь, где известны результаты по верхним границам для смещения maxbias и функции ограниченного влияния, см., например, @xcite для общего случая и @xcite для аддитивных моделей. поэтому здесь мы рассмотрим случай методов, основанных на регуляризованном ядре, основанных на общей выпуклой функции непрерывных потерь и Липшица, на общем ядре и на классическом регуляризирующем члене @ xmath1 для некоторого @ xmath2, который является штрафом за гладкость, но не штрафом за разреженность, см., например, @xcite. такие методы, основанные на регуляризованном ядре, в настоящее время часто называются машинами опорных векторов (svm), хотя исторически для таких методов использовалась нотация, основанная на специальной функции потери шарниров, и только для специальных ядер мы ссылаемся на @xcite.    в этой статье мы рассматриваем открытый вопрос, может ли svm с аддитивным ядром обеспечить существенно лучшую скорость обучения в больших размерностях, чем svm с общим ядром, скажем, классическим гауссовым ядром rbf, если выполняется предположение о аддитивной модели. наш ведущий пример описывает скорость обучения для квантильной регрессии, основанной на непрерывной, но недифференцируемой функции потерь в пинболе Липшица, которая в литературе также называется контрольной функцией, смотрите, например, @xcite и @xcite для параметрической квантильной регрессии и @xcite, @xcite и @xcite для квантильной регрессии на основе ядра. мы не будем рассматривать вопрос о том, как проверить, выполняется ли предположение аддитивной модели, поскольку это было бы темой отдельной статьи. конечно, практическим подходом могло бы быть соответствие обеих моделей и сравнение их рисков, оцененных по тестовым данным. по той же причине мы также не будем рассматривать разреженность. согласованность машин опорных векторов, генерируемых аддитивными ядрами для аддитивных моделей, была рассмотрена в @xcite. в этой статье мы устанавливаем скорость обучения для этих алгоритмов. давайте вспомним фреймворк с полным разделяемым метрическим пространством @xmath3 в качестве входного пространства и замкнутым подмножеством @xmath4 из @xmath5 в качестве выходного пространства. мера вероятности Бореля @xmath6 на @xmath7 используется для моделирования задачи обучения, а независимая и идентично распределенная выборка @xmath8 составляется в соответствии с @xmath6 для обучения. функция потерь @xmath9 используется для измерения качества функции прогнозирования @xmath10 по локальной ошибке @xmath11. _ на протяжении всей статьи мы предполагаем, что @ xmath12 измеримо, @ xmath13 выпукло относительно третьей переменной и равномерно непрерывно по Липшицу, удовлетворяющее @ xmath14 с конечной константой @xmath15. _ рассматриваемые здесь машины опорных векторов (svm ) представляют собой основанные на ядре схемы регуляризации в воспроизводящем гильбертовом пространстве ядра (rkhs) @xmath0, сгенерированном ядром mercer @xmath16. со смещенной функцией потерь @xmath17, введенной для работы даже с распределениями с тяжелыми хвостами, как @xmath18, они принимают форму @xmath19, где для общей меры Бореля @xmath20 на @xmath21 функция @xmath22 определяется @xmath23, где @xmath24 является параметром регуляризации. идея сдвига функции потерь имеет долгую историю, смотрите, например, @xcite в контексте m - оценок. в @xcite было показано, что @xmath22 также является минимизатором следующей задачи оптимизации, включающей исходную функцию потерь @xmath12, если минимизатор существует: @xmath25 рассматриваемая нами аддитивная модель состоит из _ разложения входного пространства _ @xmath26 с каждым @xmath27 полным разделимым метрическим пространством и _ гипотезой пространство _ @xmath28, где @xmath29 - это набор функций @xmath30, каждая из которых также идентифицируется как отображение @xmath31 от @xmath3 до @xmath5. следовательно, функции из @xmath32 принимают аддитивную форму @xmath33. мы упоминаем, что здесь, строго говоря, существует проблема с обозначением, потому что в предыдущей формуле каждое количество @xmath34 является элементом множества @xmath35, которое является подмножеством полного входного пространства @xmath36, @xmath37, тогда как в определении sample @xmath8 каждое количество @xmath38 является элементом из полного пространства ввода @xmath36, где @xmath39. поскольку эти обозначения будут использоваться только в разных местах и поскольку мы не ожидаем каких-либо недоразумений, мы считаем, что это обозначение проще и интуитивно понятнее, чем указание этих величин разными символами. аддитивное ядро @xmath40 определено в терминах mercer kernels @xmath41 в @xmath27 как @xmath42 оно генерирует rkhs @xmath0, который может быть записан в терминах rkhs @xmath43, сгенерированного @xmath41 в @xmath27, соответствующего форме ( [ additive ] ) как @xmath44 с нормой, заданной @xmath27 . @xmath45 норма @xmath46 удовлетворяет @xmath47 чтобы проиллюстрировать преимущества аддитивных моделей, мы приводим два примера сравнения аддитивности с ядрами продукта. первый пример имеет дело с гауссовскими ядрами rbf. все доказательства будут приведены в разделе [ proofsection ]. [ gaussadd ] пусть @xmath48, @xmath49 $ ] и @xmath50 ^ 2.$ ] пусть @xmath51 и @xmath52.\\ ] ] аддитивное ядро @xmath53 задается @xmath54 кроме того, ядро продукта @xmath55 является стандартным гауссовым ядром, заданным @xmath56, определяющим гауссову функцию @xmath57 на @xmath58 ^ 2 $ ] зависит только от одной переменной с помощью @xmath59, затем @xmath60, но @xmath61, где @xmath62 обозначает rkhs, сгенерированные стандартным гауссовским ядром rbf @xmath63. второй пример касается ядер Соболева. [ sobolvadd ] пусть @xmath64, @xmath65 $ ] и @xmath58^s.$ ] пусть @xmath66 : = \\bigl\\{u\\в l_2([0,1 ] ) ; d^\\alpha u \\в l_2([0,1 ] ) \\mbox{~для~всех~}|\\alpha|\\le 1\\bigr\\}\\ ] ] - пространство Соболева, состоящее из всех интегрируемых в квадрат одномерных функций, производная которых также интегрируема в квадрат. это rkhs с ядром mercer @xmath67, определенным в @xmath68 ^ 2 $ ]. если мы примем все ядра mercer @xmath69 за @xmath67, то @xmath70 $ ] для каждого @xmath71. аддитивное ядро @xmath72 также является ядром Мерсера и определяет rkhs @xmath73\\right\\}.\\ ] ] однако многомерное пространство Соболева @xmath74 ^ s) $ ], состоящее из всех интегрируемых в квадрат функций, частные производные которых все интегрируемы в квадрат, содержит разрывные функции и не является rkhs. обозначим предельное распределение @xmath6 на @xmath27 как @xmath75. в предположении, что @xmath76 для каждого @xmath71 и что @xmath43 является плотным в @xmath29 в @xmath77-метрике, в @xcite было доказано, что @xmath78 с вероятностью до тех пор, пока @xmath79 удовлетворяет @xmath80 и @xmath81. остальная часть статьи имеет следующую структуру. раздел [ ratessection ] содержит наши основные результаты по скорости обучения для svm на основе аддитивных ядер. скорости обучения для квантильной регрессии рассматриваются как важные частные случаи. раздел [comparisonsection ] содержит сравнение наших результатов с другими показателями обучения, опубликованными недавно. раздел [ proofsection ] содержит все доказательства и некоторые результаты, которые могут быть интересны сами по себе. в этой статье мы приводим некоторые показатели обучения для машин опорных векторов, генерируемых аддитивными ядрами для аддитивных моделей, которые помогают улучшить количественное понимание, представленное в @ xcite. показатели соответствуют асимптотическому поведению избыточного риска @xmath82 и принимают форму @xmath83 с @xmath84. они будут сформулированы при трех видах условий, включающих пространство гипотез @xmath0, меру @xmath6, потери @xmath12 и выбор параметра регуляризации @xmath85. первое условие касается способности аппроксимации пространства гипотез @xmath0. поскольку выходная функция @xmath19 берется из пространства гипотез, скорость обучения алгоритма обучения зависит от способности аппроксимации пространства гипотез @xmath0 относительно оптимального риска @xmath86, измеряемого следующей ошибкой аппроксимации. [ defapprox ] ошибка аппроксимации тройки @xmath87 определяется как @xmath88 чтобы оценить ошибку аппроксимации, мы делаем предположение о минимизаторе риска @xmath89 для каждого @xmath90, определяем интегральный оператор @xmath91, связанный с ядром @xmath41 с помощью @xmath92, мы упоминаем, что @xmath93 является компактный и позитивный оператор на @xmath94. следовательно, мы можем найти его нормализованные собственные пары @xmath95 такие, что @xmath96 является ортонормированным базисом @xmath94 и @xmath97 как @xmath98. исправьте @xmath99. тогда мы можем определить @xmath100-ю степень @xmath101 из @xmath93 с помощью @xmath102 это положительный и ограниченный оператор, и его диапазон четко определен. предположение @xmath103 означает, что @xmath104 находится в этом диапазоне. [ предположение 1 ] мы предполагаем, что @xmath105 и @xmath106, где для некоторого @xmath107 и каждого @xmath108, @xmath109 является функцией вида @xmath110 с некоторым @xmath111. случай @xmath112 предположения [ assumption1 ] означает, что каждый @xmath113 находится в rkhs @xmath43. a стандартным условием в литературе (например, @xcite) для достижения затуханий вида @xmath114 для ошибки аппроксимации ( [ approxerrordef ] ) является @xmath115 с некоторым @xmath116. здесь оператор @xmath117 определяется @xmath118 в общем, это не может быть записано в аддитивной форме. однако пространство гипотез ( [ аддитивное ] ) принимает аддитивную форму @xmath119. поэтому для нас естественно ввести аддитивное выражение @xmath120 для целевой функции @xmath121 с компонентными функциями @xmath113, удовлетворяющими условию мощности @xmath110. приведенное выше естественное предположение приводит к технической трудности при оценке ошибки аппроксимации: функция @xmath113 не имеет прямой связи с предельным распределением @xmath122, спроецированным на @xmath27, следовательно, существующие методы в литературе (например, @xcite) не могут быть применены напрямую. обратите внимание, что в пространстве продукта @xmath123 нет естественной меры вероятности, спроецированной из @xmath6, и риск в @xmath124 не определен.    наша идея преодолеть эту трудность состоит в том, чтобы ввести промежуточную функцию @xmath125. это может не свести к минимуму риск (который даже не определен). однако она хорошо аппроксимирует компонентную функцию @xmath113. когда мы суммируем такие функции @xmath126, мы получаем хорошую аппроксимацию целевой функции @xmath121 и, таким образом, хорошую оценку ошибки аппроксимации. это первая новинка статьи. [ approxerrorthm ] в предположении [ assumption1 ] у нас есть @xmath127, где @xmath128 - константа, заданная @xmath129 второе условие для наших темпов обучения - это емкость пространства гипотез, измеряемая @xmath130 - эмпирическими числами покрытия.    пусть @xmath131 - набор функций на @xmath21 и @xmath132 для каждого @xmath133 * покрывающее число @xmath131 * относительно эмпирической метрики @xmath134, заданной @xmath135, определяется как @xmath136, а * @xmath130-эмпирическое покрывающее число * из @xmath137 определяется как @xmath138 [ предположение 2 ] мы предполагаем @ xmath139 и что для некоторых @xmath140, @xmath141 и каждого @xmath142 эмпирическое число покрытия @xmath130 единичного шара @ xmath43 удовлетворяет @xmath143 вторая новизна этой статьи состоит в том, чтобы заметить, что аддитивный характер пространства гипотез приводит к следующему хорошая оценка с независимым от размерности показателем степени для покрывающих чисел шаров пространства гипотез @ xmath0, которая будет доказана в разделе [ samplesection ]. [ capacitythm ] в предположении [ assumption2 ] для любых @xmath144 и @xmath145 у нас есть @xmath146 граница для покрывающих чисел, указанная в теореме [ capacitythm ], является специальной: мощность @xmath147 не зависит от числа @xmath148 компонентов в аддитивной модели. хорошо известно @xcite в литературе о функциональных пространствах, что покрывающие числа шаров пространства Соболева @ xmath149 на кубе @ xmath150 ^ s$ ] евклидова пространства @xmath151 с индексом регулярности @xmath152 имеют следующее асимптотическое поведение с @xmath153 : @xmath154 здесь степень @xmath155 линейно зависит от размерности @xmath148. аналогичные зависящие от размерности оценки для покрывающих чисел rkhss, связанных с гауссовскими rbf - ядрами, можно найти в @xcite. специальная граница в теореме [capacitythm ] демонстрирует преимущество аддитивной модели с точки зрения емкости пространства аддитивных гипотез. третье условие для наших темпов обучения касается уровня шума в measure @xmath6 по отношению к пространству гипотез. прежде чем сформулировать общее условие, мы рассмотрим частный случай квантильной регрессии, чтобы проиллюстрировать наши общие результаты. пусть @xmath156 - параметр квантиля. функция квантильной регрессии @xmath157 определяется ее значением @xmath158 как @xmath159-квантиль @xmath160, т.е. значение @xmath161, удовлетворяющее @xmath162 схема регуляризации для квантильной регрессии, рассматриваемая здесь, принимает форму ( [ algor ] ) с функцией потерь @xmath12, заданной потерями в пинболе как @xmath163 условие шума в @xmath6 для квантильной регрессии определено в @xcite следующим образом. с этой целью пусть @xmath164 будет мерой вероятности для @xmath165 и @xmath166. тогда действительное число @xmath167 называется @xmath159-квантилем @xmath164, тогда и только тогда, когда @xmath167 принадлежит множеству @xmath168\\bigr ) \\ge \\tau \\mbox{~~и~~~ } q\\bigl([t, \\infty)\\bigr ) \\ge 1-\\tau\\bigr\\}\\,.\\] ] хорошо известно, что @xmath169 - это компактный интервал. [ noisecond ] пусть @xmath166.    1. говорят, что мера вероятности @xmath164 на @xmath165 имеет * @xmath159-квантиль типа @xmath170 *, если существует @xmath159-квантиль @xmath171 и константа @xmath172, такая, что для всех @xmath173 $ ] мы имеем @xmath174 2. пусть @xmath175 $ ]. мы говорим, что мера вероятности @xmath20 на @xmath176 имеет * @xmath159-квантиль @xmath177-средний тип @xmath170 * если мера условной вероятности @xmath178 имеет @xmath179- почти наверняка @xmath159-квантиль типа @xmath170 и функция @xmath180, где @xmath181 является константой, определенной в части (1), удовлетворяющей @xmath182. можно показать, что распределение @xmath164, имеющее @xmath159-квантиль типа @xmath170, имеет уникальный @xmath159-квантиль @xmath183. более того, если @xmath164 имеет плотность лебега @xmath184, то @xmath164 имеет @xmath159-квантиль типа @xmath170, если @xmath184 ограничен от нуля на @xmath185 $ ], поскольку мы можем использовать @xmath186\\}$ ] в ( [ tauquantileoftype2formula ] ). это предположение достаточно общее, чтобы охватить многие распределения, используемые в параметрической статистике, такие как гауссово, студенческое s @xmath187 и логистические распределения (с @xmath188), гамма- и логарифмически нормальные распределения ( с @xmath189 ), а также унифицированные и бета-версии дистрибутивов ( с @xmath190 $ ] ). следующая теорема, которая будет доказана в разделе [ proofsection ], дает скорость обучения для схемы регуляризации ( [ algor ] ) в частном случае квантильной регрессии. [ квантилет ] предположим, что @xmath191 почти наверняка соответствует некоторой константе @xmath192, и что каждое ядро @xmath41 равно @xmath193 с @xmath194 для некоторого @xmath195. если предположение [ assumption1 ] справедливо для @xmath112, а @xmath6 имеет @xmath159-квантиль @xmath177-средний тип @xmath170 для некоторого @xmath196 $], то, взяв @xmath197, для любых @xmath198 и @xmath199, с уверенностью, по крайней мере, @xmath200, мы имеем @xmath201, где @xmath202 является константой, независимой от @xmath203 и @xmath204 и @xmath205 пожалуйста, обратите внимание, что показатель @xmath206, заданный ( [ quantilerates2 ] ) для скорости обучения в ( [ quantilerates ] ), не зависит от уровня квантиля @xmath159, от количества @xmath148 аддитивных компонентов в @xmath207 и от измерения @xmath208 и @xmath209 также отмечают, что @xmath210, если @xmath211, и @xmath212, если @xmath213. поскольку @xmath214 может быть сколь угодно близко к @xmath215, скорость обучения, которая не зависит от размерности @xmath216 и задается теоремой [ quantilethm ], близка к @xmath217 для больших значений @xmath177 и близка к @xmath218 или лучше, если @xmath211.      чтобы определить наши общие показатели обучения, нам нужно предположение о _ границе дисперсии - математического ожидания _, которое аналогично определению [ noisecond ] в частном случае квантильной регрессии. [допущение3 ] мы предполагаем, что существует показатель степени @xmath219 $ ] и положительная константа @xmath220, такая, что предположение @xmath221 [ допущение3 ] всегда справедливо для @xmath222. если тройка @xmath223 удовлетворяет некоторым условиям, показатель степени @xmath224 может быть больше. например, когда @xmath12 - это потеря в пинболе ( [ pinloss ] ), а @xmath6 имеет @xmath159-квантиль @xmath177-среднее значение типа @xmath225 для некоторых @xmath196 $ ] и @xmath226, как определено в @xcite, тогда @xmath227. [ mainratesthm ] предположим, что @xmath228 почти наверняка ограничен константой @xmath229. в соответствии с предположениями [assumption1 ]-[ assumption3 ], если мы возьмем @xmath198 и @xmath230 для некоторого @xmath231, то для любого @xmath232 с уверенностью, по крайней мере, @xmath200, мы получим @xmath233, где @xmath234 задается @xmath235, а @xmath202 является константой, независимой от @xmath203 или @xmath204 ( должно быть явно указано в доказательстве ). теперь мы добавим некоторые теоретические и численные сравнения наших показателей обучаемости с данными из литературы. как уже упоминалось во введении, некоторыми причинами популярности аддитивных моделей являются гибкость, повышенная интерпретируемость и (часто) меньшая подверженность проклятию больших размерностей. следовательно, важно проверить, выгодно ли скорость обучения, указанная в теореме [ mainratesthm ] в предположении аддитивной модели, сравнивается с (по существу) оптимальными скоростями обучения без этого предположения. другими словами, нам нужно продемонстрировать, что основная цель этой статьи достигается теоремой [quantilethm ] и теоремой [ mainratesthm], т.е. что svm, основанный на аддитивном ядре, может обеспечить существенно лучшую скорость обучения в больших размерностях, чем svm с общим ядром, скажем, классическим гауссовым rbf ядро, при условии, что выполняется предположение об аддитивной модели. наша скорость обучения в теореме [quantilethm ] является новой и оптимальной в литературе по svm для квантильной регрессии. большинство показателей обучения в литературе по svm для квантильной регрессии приведены для прогнозируемых выходных функций @xmath236, в то время как хорошо известно, что прогнозы улучшают показатели обучения @xcite. здесь оператор проекции @xmath237 определен для любой измеримой функции @xmath10 с помощью @xmath238 иногда это называется отсечением. такие результаты приведены в @xcite. например, в предположениях, что @xmath6 имеет @xmath159-квантиль @xmath177-средний тип @xmath170, условие ошибки аппроксимации ( [ approxerrorb ] ) выполняется для некоторого @xmath239, и что для некоторых констант @xmath240 последовательность собственных значений @xmath241 интегрального оператора @xmath117 удовлетворяет @xmath242 для каждого @xmath243, в @xcite было показано, что с уверенностью, по крайней мере, @xmath200, @xmath244, где @xmath245 здесь параметр @xmath246 измеряет мощность rkhs @xmath247 и играет ту же роль, что и половина параметра @xmath147 в предположении 2. для ядра @xmath193 и @xmath112 можно выбрать произвольно малыми @xmath246 и @xmath147, а указанный выше индекс мощности @xmath248 можно принять за @xmath249. скорость обучения в теореме [ quantilethm ] может быть улучшена путем ослабления предположения 1 до условия гладкости Соболева для @ xmath121 и условия регулярности для предельного распределения @xmath250. например, можно использовать гауссово ядро @xmath251 в зависимости от размера выборки @xmath203 и @xcite для достижения условия ошибки аппроксимации ( [ approxerrorb ] ) для некоторого @xmath252. это сделано для квантильной регрессии в @xcite. поскольку нас в основном интересуют аддитивные модели, мы не будем обсуждать такое расширение. [ gaussmore ] пусть @xmath48, @xmath49 $ ] и @xmath50 ^ 2.$ ] пусть @xmath51 и аддитивное ядро @xmath72 задаются через ( [ gaussaddform ] ) с @xmath253 в примере [ gaussadd ] как @xmath52.\\ ] ] если функция @xmath121 задается через ( [ gaussfcn ] ), @xmath191 почти наверняка для некоторой константы @xmath192, а @xmath6 имеет @xmath159-квантиль @xmath177-средний тип @xmath170 для некоторого @xmath196 $ ], затем, взяв @xmath197, для любых @xmath145 и @xmath199, ( [ квантилирует ] ) выполняется с уверенность, по крайней мере, в xmath200.    неизвестно, может ли указанная выше скорость обучения быть получена с помощью существующих подходов в литературе (например, @xcite) даже после проектирования. обратите внимание, что ядро в приведенном выше примере не зависит от размера выборки. было бы интересно посмотреть, существует ли какой-нибудь @xmath99 такой, что функция @xmath57, определенная с помощью ( [ gaussfcn ] ), находится в диапазоне оператора @xmath254. существование такого положительного индекса привело бы к условию ошибки аппроксимации ( [ approxerrorb ] ), см. @xcite.    давайте теперь добавим несколько численных сравнений качества наших показателей обучения, приведенных в теореме [ mainratesthm ], с данными @xcite. их следствие 4.12 дает (по существу) минимально оптимальные скорости обучения для (обрезанных) svm в контексте непараметрической квантильной регрессии с использованием одного гауссовского ядра rbf на всем входном пространстве при соответствующих предположениях о гладкости целевой функции. давайте рассмотрим случай, когда распределение @xmath6 имеет @xmath159-квантиль @xmath177-средний тип @xmath170, где @xmath255, и предположим, что применимы как следствие 4.12 в @xcite, так и наша теорема [ mainratesthm]. т.е. мы предполагаем, в частности, что @xmath6 является мерой вероятности на @xmath256 $ ] и что предельное распределение @xmath257 имеет плотность Лебега @xmath258 для некоторого @xmath259. кроме того, предположим, что функция оптимального решения @xmath260 имеет (чтобы сделать теорему [ mainratesthm ] применимой к @xmath261 $ ] ) аддитивную структуру @xmath207 с каждым @xmath104, как указано в предположении [ допущение1 ], где @xmath262 и @xmath263, с минимальным риском @xmath86 и дополнительно выполняет (чтобы сделать следствие 4.12 в @xcite применимо ) @xmath264, где @xmath265 $ ] и @xmath266 обозначает пространство Бесова с параметром гладкости @xmath267. интуитивное значение @xmath248 заключается в том, что возрастающие значения @xmath248 соответствуют повышенной гладкости. мы ссылаемся на ( * ? ? ? * и стр. 44 ) для получения подробной информации о пространствах Бесова. хорошо известно, что пространство Бесова @xmath268 содержит пространство Соболева @xmath269 для @xmath270, @xmath271 и @xmath272, и что @xmath273. мы упоминаем, что если все @ xmath41 являются подходящим образом выбранными ядрами Вендланда, то их воспроизводящие гильбертовы пространства ядра @xmath43 являются пространствами Соболева, см. ( * ? ? ? * thm. 10.35, стр. 160). кроме того, мы используем ту же последовательность регуляризующих параметров, что и в (* ? ?? 4.9, кор. 4.12), т.е. @xmath274, где @xmath275, @xmath276, @xmath277 $ ], а @xmath278 - некоторая определенная пользователем положительная константа, независимая от @xmath279. для простоты давайте исправим @xmath280. затем (* ? ? ? 4.12 ) дает показатели обучения для риска svms для @xmath159-квантильной регрессии, если одно гауссово rbf - ядро на @xmath281 используется для @xmath159-квантильных функций @xmath177-среднего типа @xmath170 с @xmath255, которые имеют порядок @xmath282, следовательно скорость обучения в теореме [ quantilethm ] лучше, чем в ( * ? ? ? 4.12 ) в этой ситуации, если @xmath283 при условии, что предположение аддитивной модели справедливо. в таблице [ table1 ] перечислены значения @xmath284 из ( [ explicitratescz2 ] ) для некоторых конечных значений измерения @xmath216, где @xmath285. все эти значения @xmath284 положительны, за исключением @xmath286 или @xmath287. это контрастирует с соответствующим показателем скорости обучения на ( * ? ? * кор. 4.12 ), поскольку таблица @xmath288 [ таблица2 ] и рисунки [ рис. 1 ]-[ рис. 2 ] дают дополнительную информацию о пределе @xmath289. конечно, более высокие значения показателя степени указывают на более высокие темпы сходимости. очевидно, что svm, основанный на аддитивном ядре, имеет значительно более высокую скорость сходимости в более высоких измерениях @xmath216 по сравнению с svm, основанным на одном гауссовском ядре rbf, определенном во всем входном пространстве, конечно, при условии, что аддитивная модель верна. цифры, по-видимому, указывают на то, что наша скорость обучения по теореме [ mainratesthm ], вероятно, неоптимальна для малых измерений. однако основное внимание в настоящей статье уделяется большим измерениям. 

аддитивные модели играют важную роль в полупараметрической статистике. в этой статье приведены показатели обучения для методов, основанных на регуляризованном ядре, для аддитивных моделей. эти скорости обучения выгодно отличаются, в частности, в больших измерениях, от недавних результатов по оптимальным скоростям обучения для чисто непараметрической регуляризованной квантильной регрессии на основе ядра с использованием ядра радиальной базисной функции Гаусса, при условии, что предположение об аддитивной модели справедливо. дополнительно представлен конкретный пример, показывающий, что гауссова функция, зависящая только от одной переменной, лежит в воспроизводящем гильбертовом пространстве ядра, генерируемом аддитивным гауссовым ядром, но не принадлежит воспроизводящему гильбертову пространству ядра, генерируемому многомерным гауссовым ядром с той же дисперсией.    * ключевые слова и фразы. * аддитивная модель, ядро, квантильная регрессия, полупараметрическая, скорость сходимости, метод опорных векторов.